{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b2ea032d525cef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduzione a LangChain\n",
    "\n",
    "***Argomenti:***\n",
    "* Runnable\n",
    "* Prompt Template\n",
    "* Memoria Sequenziale\n",
    "* Parserizzazione di Input e Output"
   ]
  },
  {
   "cell_type": "code",
   "id": "b69073df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:10.690946Z",
     "start_time": "2025-10-30T11:48:10.687096Z"
    }
   },
   "source": [
    "import os\n",
    "OPENAI_KEY = os.getenv(\"openai_key\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ae45715b13d443d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:30.427602Z",
     "start_time": "2025-10-30T11:48:27.736519Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI   # pip install langchain-openai\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_KEY, \n",
    "    model_name = \"gpt-4o-mini\",\n",
    "    temperature=0.7, #livello di creatività/casualità nelle risposte (0: minima variabilità, 1: massima variabilità)\n",
    "    max_tokens=1024, #numero massimo di token che il modello può generare in una sola risposta\n",
    "    request_timeout=30,\n",
    "    # ---\n",
    "    # Possiamo usare questo connettore anche per modelli in locale\n",
    "    # ---\n",
    "    #model=\"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    #base_url = \"http://127.0.0.1:8000/v1\",\n",
    ")\n",
    "\n",
    "# Le info sui pricing dei modelli si possono trovare qui: \n",
    "# https://platform.openai.com/docs/pricing"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Cloud\\GitHub\\LLMs_MasterClass\\lc1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "698425b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:31.530422Z",
     "start_time": "2025-10-30T11:48:31.160420Z"
    }
   },
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "#  init_chat model\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"openai_key\"),\n",
    "    temperature=0.7,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "9f7e02bb220c9b50",
   "metadata": {},
   "source": [
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRXuDvlNDmDGF5QwPETEs3eh7RHNGmKBpgwyw&s\">  \n",
    "  \n",
    "# OpenAI GPT-4o mini\n",
    "**Generative Pre-trained Transformer 4o mini** (GPT-4o mini) è una versione compatta della serie di modelli GPT-4, sviluppata da OpenAI nel 2024. \n",
    "\n",
    "Il modello GPT-4o mini è stato progettato per bilanciare efficienza e prestazioni, offrendo una capacità inferiore rispetto ai modelli completi di GPT-4, ma con una velocità e un costo migliorati per applicazioni che richiedono risposte rapide e consumo limitato di risorse. Ideale per implementazioni su larga scala in ambienti con restrizioni computazionali o per casi d'uso che non necessitano della massima potenza offerta dai modelli più grandi.\n",
    "\n",
    "*Caratteristiche:*\n",
    "* Contesto di 128K tokens\n",
    "* Output di 16384 token massimo\n",
    "* Ottimizzato per l'efficienza in ambienti a risorse limitate\n",
    "* Addestrato con tecniche di RLHF (Reinforcement Learning from Human Feedback)\n",
    "* Eccellente per applicazioni che richiedono un bilanciamento tra costo, velocità e prestazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb51e59495905",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning from Human Feedback\n",
    "\n",
    "Nel **RLHF (Reinforcement Learning from Human Feedback)** il modello genera delle risposte che vengono valutate poi dagli esseri umani, che forniscono un feedback di qualità (es. preferenze su quale risposta è migliore). Questo feedback viene trasformato in \"ricompense\" tramite un modello di ricompensa (similmente a come accade nel Reinforcement Learning), che assegna punteggi alle risposte generate dall'AI in base a quanto si avvicinano alle preferenze umane.\n",
    "\n",
    "Gli algoritmi di apprendimento per rinforzo utilizzano questi punteggi di ricompensa per aggiornare i pesi del modello in modo iterativo. A differenza del fine-tuning statico, il RLHF consente al modello di evolversi e migliorare continuamente, generando risposte sempre più allineate con i valori e le preferenze umane, seguendo un processo di adattamento graduale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277da601",
   "metadata": {},
   "source": [
    "<hr>    \n",
    "<img src=\"imgs/seq_memory.png\" width=\"800\">    \n",
    "    \n",
    "[https://python.langchain.com/docs/concepts/messages/#langchain-messages](https://python.langchain.com/docs/concepts/messages/#langchain-messages)"
   ]
  },
  {
   "cell_type": "code",
   "id": "38330f141f9c6c6f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:36.216564Z",
     "start_time": "2025-10-30T11:48:35.159054Z"
    }
   },
   "source": [
    "# LangChain si occupa autonomamente di gestire i vari tag (<SOS>, <EOS>, etc) corretti per il modello che stiamo utilizzano\n",
    "# ---\n",
    "# In questo caso il nostro output sarà un AIMessage che conterrà un content e una serie di metadati associati\n",
    "# Tutti i messaggi hanno sempre la chiave content da cui recuperarci il corpo del messaggio\n",
    "model.invoke(\"ciao sono Gennaro\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao Gennaro! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CWLQtrd6Jmssu1HjrAc8S0mj99pbZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--97580cc3-bb4e-4445-bced-888481a18ed3-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "ee908358",
   "metadata": {},
   "source": [
    "<hr>    \n",
    "\n",
    "<img src=\"imgs/prompt_template.png\" width=\"800\">    \n",
    "    \n",
    "[https://python.langchain.com/docs/how_to/#prompt-templates](https://python.langchain.com/docs/how_to/#prompt-templates)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8437aefa5d85256",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:37.513394Z",
     "start_time": "2025-10-30T11:48:37.486823Z"
    }
   },
   "source": [
    "# prompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"Act as a world class Machine Learning engineer. Use italian language. Ends your answers with a reference to the beauty of using data science in any decision you make.\"),\n",
    "    (\"user\", \"{pippo}\"),\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | model"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b4f01ffb-98fe-4833-95b6-a4a3a265908b",
   "metadata": {},
   "source": [
    "## Interfaccia Runnable\n",
    "\n",
    "Per rendere più semplice la creazione di catene di eventi/esecuzione anche molto complesse i componenti di LangChain implementano tutti un protocollo \"runnable\" tramite un'interfaccia comune che permette di usare qualsiasi componente in modo standard; di seguito sono elencati i 3 principali metodi:\n",
    "\n",
    "* **stream** - inviare risposte parziali mentre vengono generate\n",
    "* **invoke** - eseguire la catena su un input\n",
    "* **batch** - esecuzione della catena su più input\n",
    "\n",
    "Uno dei vantaggi delle interfacce Runnable è dato dal fatto che dei componenti *runnable* possono essere concatenati in sequenze di esecuzione, facendo in modo che, automaticamente, gli output di un componente possano entrare in input ad un altro; il comando *pipe* `|` serve a questo e permette, nella sintassi LCEL (LangChain Expression Language) di creare componenti runnable partendo da altri componenti runnable, configurandoli in una sequenza di componenti che agiranno sinergicamente."
   ]
  },
  {
   "cell_type": "code",
   "id": "54e68f8f-490e-4bc8-8479-2cd861c9483f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:41.349556Z",
     "start_time": "2025-10-30T11:48:41.345247Z"
    }
   },
   "source": [
    "prompt.invoke({\"pippo\": \"elencami i primi 3 pianeti del sistema solare\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Act as a world class Machine Learning engineer. Use italian language. Ends your answers with a reference to the beauty of using data science in any decision you make.', additional_kwargs={}, response_metadata={}), HumanMessage(content='elencami i primi 3 pianeti del sistema solare', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "aa049fbcb0821047",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-30T11:48:46.672650Z",
     "start_time": "2025-10-30T11:48:42.855099Z"
    }
   },
   "source": [
    "chain.invoke({\"pippo\": \"elencami i primi 3 pianeti del sistema solare\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I primi tre pianeti del sistema solare, partendo dal sole, sono:\\n\\n1. Mercurio\\n2. Venere\\n3. Terra\\n\\nQuesti pianeti presentano caratteristiche uniche e affascinanti. La bellezza di esplorare l'universo, così come l'analisi dei dati, ci permette di prendere decisioni più informate e di apprezzare la complessità del nostro universo.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 56, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CWLR1S5nwk7nGdSNYy7yvM0y4CeWr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--8b552260-59cf-4138-a379-377313e310c1-0', usage_metadata={'input_tokens': 56, 'output_tokens': 88, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:49:21.867350Z",
     "start_time": "2025-10-30T11:49:19.518328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# streaming degli output\n",
    "\n",
    "answer = None\n",
    "for chunk in model.stream(\"elencami i primi 3 pianeti del sistema solare\"):\n",
    "    answer = chunk if answer is None else answer + chunk\n",
    "    print(answer.text, \"\\n\\n     -----\\n\\n\")\n",
    "\n",
    "print(\"-\" * 10, \"\\n\", answer.content)"
   ],
   "id": "18e58d6d5bc96ac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pian \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema sol \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      " \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1 \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. ** \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Merc \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      " \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2 \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. ** \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Ven \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      " \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3 \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. ** \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Ter \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      " \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulterior \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su cias \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pian \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fam \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, famm \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere! \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere! \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere! \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere! \n",
      "\n",
      "     -----\n",
      "\n",
      "\n",
      "---------- \n",
      " I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:51:09.277752Z",
     "start_time": "2025-10-30T11:51:07.525800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gestione degli eventi durante lo streaming\n",
    "\n",
    "async for event in model.astream_events(\"elencami i primi 3 pianeti del sistema solare\"):\n",
    "\n",
    "    if event[\"event\"] == \"on_chat_model_start\":\n",
    "        print(f\"Input: {event['data']['input']}\\n\\n\")\n",
    "\n",
    "    elif event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(f\"Token: {event['data']['chunk'].text}\")\n",
    "\n",
    "    elif event[\"event\"] == \"on_chat_model_end\":\n",
    "        print(f\"\\n\\nFull message:\\n{event['data']['output'].text}\")\n",
    "\n",
    "    else:\n",
    "        pass"
   ],
   "id": "7da3070cfd8b6329",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: elencami i primi 3 pianeti del sistema solare\n",
      "\n",
      "\n",
      "Token: \n",
      "Token: I\n",
      "Token:  primi\n",
      "Token:  tre\n",
      "Token:  pian\n",
      "Token: eti\n",
      "Token:  del\n",
      "Token:  sistema\n",
      "Token:  sol\n",
      "Token: are\n",
      "Token: ,\n",
      "Token:  part\n",
      "Token: endo\n",
      "Token:  dal\n",
      "Token:  Sole\n",
      "Token: ,\n",
      "Token:  sono\n",
      "Token: :\n",
      "\n",
      "\n",
      "Token: 1\n",
      "Token: .\n",
      "Token:  **\n",
      "Token: Merc\n",
      "Token: urio\n",
      "Token: **\n",
      "\n",
      "Token: 2\n",
      "Token: .\n",
      "Token:  **\n",
      "Token: Ven\n",
      "Token: ere\n",
      "Token: **\n",
      "\n",
      "Token: 3\n",
      "Token: .\n",
      "Token:  **\n",
      "Token: Ter\n",
      "Token: ra\n",
      "Token: **\n",
      "Token:  \n",
      "\n",
      "\n",
      "Token: Se\n",
      "Token:  hai\n",
      "Token:  bisogno\n",
      "Token:  di\n",
      "Token:  ulterior\n",
      "Token: i\n",
      "Token:  informazioni\n",
      "Token:  su\n",
      "Token:  cias\n",
      "Token: cun\n",
      "Token:  pian\n",
      "Token: eta\n",
      "Token: ,\n",
      "Token:  fam\n",
      "Token: m\n",
      "Token: elo\n",
      "Token:  sapere\n",
      "Token: !\n",
      "Token: \n",
      "Token: \n",
      "Token: \n",
      "\n",
      "\n",
      "Full message:\n",
      "I primi tre pianeti del sistema solare, partendo dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra** \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T11:55:30.892438Z",
     "start_time": "2025-10-30T11:55:15.846651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# esempio di chiamate in batch di più richieste\n",
    "# da servire in parallelo (quando il sistema lo consente)\n",
    "# con verifica del tempo di esecuzione\n",
    "\n",
    "import time\n",
    "\n",
    "queries = [\n",
    "    \"elencami i primi 3 pianeti del sistema solare\",\n",
    "    \"elencami gli ultimi 3 pianeti del sistema solare\",\n",
    "    \"elenca tre pianeti del sistema solare\"\n",
    "]\n",
    "\n",
    "start_batch = time.time()\n",
    "responses_batch = model.batch(queries)\n",
    "end_batch = time.time()\n",
    "\n",
    "print(\"=== Risultati batch ===\")\n",
    "for r in responses_batch:\n",
    "    print(r.content)\n",
    "print(f\"\\nTempo totale batch: {end_batch - start_batch:.3f} secondi\\n\\n\")\n",
    "\n",
    "responses_single = []\n",
    "start_single = time.time()\n",
    "for q in queries:\n",
    "    resp = model.invoke(q)\n",
    "    responses_single.append(resp)\n",
    "end_single = time.time()\n",
    "\n",
    "print(\"=== Risultati singoli ===\")\n",
    "for r in responses_single:\n",
    "    print(r.content)\n",
    "print(f\"\\nTempo totale chiamate singole: {end_single - start_single:.3f} secondi\")"
   ],
   "id": "7457561373a814f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Risultati batch ===\n",
      "I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono:\n",
      "\n",
      "1. **Mercurio**\n",
      "2. **Venere**\n",
      "3. **Terra**\n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammi sapere!\n",
      "I tre pianeti più esterni del sistema solare, partendo dal Sole, sono:\n",
      "\n",
      "1. **Uranus** - Il settimo pianeta del sistema solare, noto per il suo colore blu-verde dovuto alla presenza di metano nell'atmosfera e per il suo asse di rotazione fortemente inclinato.\n",
      "\n",
      "2. **Nettuno** - L'ottavo pianeta, spesso descritto come il pianeta più lontano dal Sole. Anch'esso ha una colorazione blu, sempre a causa del metano nell'atmosfera, e presenta intense tempeste atmosferiche.\n",
      "\n",
      "3. **Plutone** - Sebbene non sia più considerato un pianeta principale (è stato riclassificato come pianeta nano nel 2006), Plutone è spesso incluso nelle discussioni sui pianeti del sistema solare. Si trova oltre Nettuno e ha un'orbita altamente ellittica.\n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammelo sapere!\n",
      "Certo! Ecco tre pianeti del sistema solare:\n",
      "\n",
      "1. Mercurio\n",
      "2. Venere\n",
      "3. Terra\n",
      "\n",
      "Se hai bisogno di ulteriori informazioni sui pianeti o su altri argomenti, fammelo sapere!\n",
      "\n",
      "Tempo totale batch: 5.810 secondi\n",
      "\n",
      "\n",
      "=== Risultati singoli ===\n",
      "I primi tre pianeti del sistema solare, partendo dal Sole, sono:\n",
      "\n",
      "1. **Mercurio** - È il pianeta più vicino al Sole ed è il più piccolo del sistema solare.\n",
      "2. **Venere** - È il secondo pianeta dal Sole e ha caratteristiche atmosferiche uniche, con temperature molto elevate.\n",
      "3. **Terra** - È il terzo pianeta dal Sole ed è l'unico noto per ospitare vita. \n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su ciascun pianeta, fammi sapere!\n",
      "I tre pianeti più distanti dal Sole nel sistema solare sono:\n",
      "\n",
      "1. **Urano** - Settimo pianeta dal Sole, noto per il suo colore blu dovuto alla presenza di metano nell'atmosfera.\n",
      "2. **Nettuno** - Ottavo pianeta dal Sole, è il pianeta più lontano del sistema solare e ha una intensa attività atmosferica.\n",
      "3. **Plutone** - Fino al 2006, Plutone era considerato il nono pianeta del sistema solare, ma poi è stato riclassificato come \"pianeta nano\" dall'Unione Astronomica Internazionale.\n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su uno di questi pianeti, non esitare a chiedere!\n",
      "Certo! Ecco tre pianeti del sistema solare:\n",
      "\n",
      "1. Mercurio\n",
      "2. Venere\n",
      "3. Terra\n",
      "\n",
      "Se hai bisogno di ulteriori informazioni su uno di questi pianeti, fammelo sapere!\n",
      "\n",
      "Tempo totale chiamate singole: 9.230 secondi\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:03:56.420924Z",
     "start_time": "2025-10-30T12:03:49.032995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# restituzione di un output appena pronto,\n",
    "# durante l'esecuzione di più input in batch\n",
    "\n",
    "start_time = time.time()\n",
    "for response in model.batch_as_completed([\n",
    "    \"Descrivi brevemente La Divina Commedia\",\n",
    "    \"2 + 2 = ?\",\n",
    "    \"Ciao, sono Enzo\"\n",
    "], config={\n",
    "        'max_concurrency': 5,  # numero limite di chiamate in parallelo - inutile in questo esempio\n",
    "    }):\n",
    "    print(response[0], time.time() - start_time, \"\\n\", response[1].content, \"\\n\\n\")"
   ],
   "id": "d1c293ed2c628ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8644392490386963 \n",
      " 2 + 2 = 4. \n",
      "\n",
      "\n",
      "2 1.201648473739624 \n",
      " Ciao Enzo! Come posso aiutarti oggi? \n",
      "\n",
      "\n",
      "0 7.384363412857056 \n",
      " \"La Divina Commedia\" è un poema epico scritto da Dante Alighieri nel XIV secolo, considerato uno dei capolavori della letteratura mondiale. L'opera è composta da tre cantiche: Inferno, Purgatorio e Paradiso, per un totale di 100 canti. \n",
      "\n",
      "La trama segue il viaggio del protagonista, Dante stesso, attraverso questi tre regni dell'aldilà, guidato inizialmente dal poeta romano Virgilio e, successivamente, da Beatrice, simbolo dell'amore divino. \n",
      "\n",
      "Nel \"Inferno\", Dante esplora i peccati e le punizioni dei dannati, descrivendo i vari cerchi dell'Inferno e le anime che vi si trovano. Nel \"Purgatorio\", si affronta il tema della redenzione e della purificazione delle anime in attesa di entrare in Paradiso. Infine, nel \"Paradiso\", Dante descrive la beatitudine dei giusti e l'unione con Dio.\n",
      "\n",
      "L'opera affronta temi profondi come la giustizia, la fede, l'amore e la ricerca della verità, utilizzando una lingua poetica ricca e simbolica. \"La Divina Commedia\" ha avuto un'enorme influenza sulla cultura e sulla letteratura, consolidando la lingua italiana e offrendo una riflessione duratura sulla condizione umana. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "d62ece24-7500-4b4b-9a94-4f1dd219dd15",
   "metadata": {},
   "source": [
    "### Ovviamente non siamo limitati ad un solo placeholder, ma possiamo usarne più di uno"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b96132d-8385-4f82-bbe9-b7ae17fe2837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:08:51.072028Z",
     "start_time": "2025-10-29T16:08:51.068400Z"
    }
   },
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"Act as a useful AI Assistant. Always answer the user question using only the {language} language, regardless of the language used by the user\"),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | model"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "658d23b4-b371-4c2e-85f6-f99251f15aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:08:57.747667Z",
     "start_time": "2025-10-29T16:08:53.170699Z"
    }
   },
   "source": [
    "chain.invoke({\"language\":\"italian\", \"query\": \"Elencami i primi 3 pianeti del sistema solare\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I primi tre pianeti del sistema solare, in ordine di distanza dal Sole, sono Mercurio, Venere e Terra.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 50, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31JHk7Cmu7GCBot7BQjsrV5KNCF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5e96ea69-3ba0-40c3-9838-b83c2b030999-0', usage_metadata={'input_tokens': 50, 'output_tokens': 26, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "8baf4940-cd5d-480d-9c31-e2e2bf204e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:08:58.750945Z",
     "start_time": "2025-10-29T16:08:57.769822Z"
    }
   },
   "source": [
    "chain.invoke({\"language\":\"english\", \"query\": \"Elencami i primi 3 pianeti del sistema solare\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first three planets of the solar system are Mercury, Venus, and Earth.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 50, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31K7zfmuRxvXLAbAjQMiNhaMcuL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--eeaae372-83ef-44aa-b289-e34d61bca934-0', usage_metadata={'input_tokens': 50, 'output_tokens': 16, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "ae27a9d8-5331-4499-bd8f-d6ee1b6ed927",
   "metadata": {},
   "source": [
    "##  Memoria Sequenziale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dae068-86b2-402a-8e14-84278aaa3326",
   "metadata": {},
   "source": [
    "Ovviamente in questo caso il modello non ha modo di accedere alla memoria della nostra conversazione."
   ]
  },
  {
   "cell_type": "code",
   "id": "a1e81889-df92-46c1-9751-bed50781658a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:00.604880Z",
     "start_time": "2025-10-29T16:09:00.600841Z"
    }
   },
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"Act as a useful AI Assistant.\"),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | model"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "bae4c470-a6d6-44db-815e-2e8a1a62efb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:02.423175Z",
     "start_time": "2025-10-29T16:09:01.844099Z"
    }
   },
   "source": [
    "chain.invoke({\"query\": \"Ciao, io mi chiamo Simone\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao Simone! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 26, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31OSZPVKjbPtqXTBCoPUuDzGIRV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--36ec627b-8272-495d-91d6-a74bb0af4f65-0', usage_metadata={'input_tokens': 26, 'output_tokens': 10, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "cbfb752c-d48b-42b7-b077-50a96dab2de7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:06.415729Z",
     "start_time": "2025-10-29T16:09:05.082066Z"
    }
   },
   "source": [
    "chain.invoke({\"query\": \"Come mi chiamo?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Non ho accesso a informazioni personali, quindi non posso sapere come ti chiami. Posso aiutarti in altro modo?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 23, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CW31RcfHaFhJ95h8gashf889KcRWr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4a2b596e-0614-4daa-a793-d474344239a4-0', usage_metadata={'input_tokens': 23, 'output_tokens': 25, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "44a4d413-44ce-41bd-bea5-98cb7eebd553",
   "metadata": {},
   "source": [
    " Per permettere al modello di \"ricordare\" informazioni dalla conversazione, bisogna introdurre il concetto di __memoria__\n",
    " \n",
    "Aggiungere la memoria significa, a conti fatti, iniettare lo storico della conversaione avuta sino a quel momento all'interno del prompt, così che ad ogni nuova richiesta che facciamo al LLM, questo riceva anche tutto lo scambio di conversazioni avute oltre che al SystemMessage e all'ultimo messaggio dell'utente"
   ]
  },
  {
   "cell_type": "code",
   "id": "998d21ef-5a1b-4dc8-b0ff-da4afcde8dff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:07.433486Z",
     "start_time": "2025-10-29T16:09:07.421711Z"
    }
   },
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts  import MessagesPlaceholder \n",
    "\n",
    "# -----------------------------------------------\n",
    "# A regime il nostro prompt sarà strutturato così:\n",
    "# > System message\n",
    "# > History Message 1\n",
    "# > History Message 2\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# > History Message N\n",
    "# > Human Message (in input)\n",
    "# -----------------------------------------------\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"Act as a useful AI Assistant. Answer using italian language\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "29405f29-82b6-48ea-9f65-49241b6f0b31",
   "metadata": {},
   "source": [
    "Facciamo una prova con una lista di messaggi fittizi che simulano uno storico di conversazione"
   ]
  },
  {
   "cell_type": "code",
   "id": "88783464-d860-419e-8f81-a7db7cf45832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:13.101715Z",
     "start_time": "2025-10-29T16:09:13.096643Z"
    }
   },
   "source": [
    "dummy_messages = [\n",
    "   (\"user\", \"Ciao, io sono Simone\"),\n",
    "   (\"assistant\", \"Ciao Simone, come posso aiutarti?\"),\n",
    "   (\"user\", \"Volevo chiederti una cosa, posso?\"),\n",
    "   (\"assistant\", \"Certamente! Dimmi pure\"),\n",
    "]\n",
    "\n",
    "prompt.invoke({\"history\":dummy_messages, \"query\": \"elencami i primi 3 pianeti del sistema solare\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Act as a useful AI Assistant. Answer using italian language', additional_kwargs={}, response_metadata={}), HumanMessage(content='Ciao, io sono Simone', additional_kwargs={}, response_metadata={}), AIMessage(content='Ciao Simone, come posso aiutarti?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Volevo chiederti una cosa, posso?', additional_kwargs={}, response_metadata={}), AIMessage(content='Certamente! Dimmi pure', additional_kwargs={}, response_metadata={}), HumanMessage(content='elencami i primi 3 pianeti del sistema solare', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "a3414b6c-76ef-4e22-a205-c7e4a9de697e",
   "metadata": {},
   "source": [
    "Ovviamente in un contesto reale, non mettiamo noi manualmente i messaggi in una lista, ma lasciamo che sia LangChain a farlo per noi.\n",
    "In questo modo abbiamo anche la possibilità di tenere separate delle sessioni di conversazioni, ognuna con la sua memoria univoca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb02d505eb6fa7",
   "metadata": {},
   "source": [
    "### RunnableWithMessageHistory\n",
    "\n",
    "**Runnable che gestisce la cronologia dei messaggi di chat per un altro Runnable.**\n",
    "\n",
    "Una *chat message history* è una sequenza di messaggi che rappresenta una conversazione.\n",
    "\n",
    "`RunnableWithMessageHistory` incapsula un altro `Runnable` e gestisce per questo la *chat message history*; è responsabile della lettura e dell'aggiornamento della cronologia dei messaggi.\n",
    "\n",
    "Deve sempre essere invocato con una configurazione (`config`) che contenga i parametri appropriati per il recupero dei messaggi corretti.\n",
    "\n",
    "Per impostazione predefinita, il `Runnable` si aspetta un singolo parametro di configurazione chiamato `session_id` di tipo stringa; questo parametro viene utilizzato per creare una nuova *chat message history* o recuperarne una esistente associata al `session_id` specificato.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1888328f-5946-4b24-8aec-0021ce771cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:18.907962Z",
     "start_time": "2025-10-29T16:09:18.902507Z"
    }
   },
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "38ddd922-b29a-424e-94e0-39a1b1b7b1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:20.918564Z",
     "start_time": "2025-10-29T16:09:19.622866Z"
    }
   },
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\": \"Ciao, mi chiamo Simone\"},\n",
    "    config={\"session_id\": \"id_1234\"}\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao Simone! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 29, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31giABfqdjBxYQMfjcCRj7hwL0C', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ea5ba668-6e2c-4863-ae58-5c07416dbc2c-0', usage_metadata={'input_tokens': 29, 'output_tokens': 10, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "7f04e861-f59a-4161-8ce3-fe28ab9cd69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:21.177178Z",
     "start_time": "2025-10-29T16:09:21.173723Z"
    }
   },
   "source": [
    "chat_map[\"id_1234\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Ciao, mi chiamo Simone', additional_kwargs={}, response_metadata={}), AIMessage(content='Ciao Simone! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 29, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31giABfqdjBxYQMfjcCRj7hwL0C', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ea5ba668-6e2c-4863-ae58-5c07416dbc2c-0', usage_metadata={'input_tokens': 29, 'output_tokens': 10, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "f329506b-7f2d-47b7-9d4b-14c377220506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:23.365161Z",
     "start_time": "2025-10-29T16:09:22.164827Z"
    }
   },
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\": \"Ciao, mi chiamo Enzo\"},\n",
    "    config={\"session_id\": \"id_5678\"}\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao Enzo! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 30, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31jI4RdhSwDe7mLoIbNKzXpFUvU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f1b40f4d-2241-4190-8480-ced116b387a7-0', usage_metadata={'input_tokens': 30, 'output_tokens': 11, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "13c7b1a5-efe1-4da7-a970-93699646bf69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:23.388748Z",
     "start_time": "2025-10-29T16:09:23.386097Z"
    }
   },
   "source": [
    "chat_map[\"id_5678\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Ciao, mi chiamo Enzo', additional_kwargs={}, response_metadata={}), AIMessage(content='Ciao Enzo! Come posso aiutarti oggi?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 30, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31jI4RdhSwDe7mLoIbNKzXpFUvU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f1b40f4d-2241-4190-8480-ced116b387a7-0', usage_metadata={'input_tokens': 30, 'output_tokens': 11, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "c4fa19d8-247c-4d74-853f-7f176b4574fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:24.991459Z",
     "start_time": "2025-10-29T16:09:24.074286Z"
    }
   },
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\": \"Come mi chiamo?\" },\n",
    "    config={\"session_id\": \"id_1234\"}\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Ti chiami Simone. Hai bisogno di aiuto con qualcos'altro?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 52, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31kuaz0QuWFbgeaoZ4rwIB3x5Tx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5254824f-8eb3-4f44-817a-ce1e99c73d22-0', usage_metadata={'input_tokens': 52, 'output_tokens': 16, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "9dfa48f5-e302-461d-9c6e-89bc13a555f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:25.860677Z",
     "start_time": "2025-10-29T16:09:25.034116Z"
    }
   },
   "source": [
    "chain_with_history.invoke(\n",
    "    {\"query\": \"Come mi chiamo?\" },\n",
    "    config={\"session_id\": \"id_5678\"}\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ti chiami Enzo! Come posso assisterti?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 54, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31lw3Shcyb5WhHTPX4lptbFdWmr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--1e585b60-40a8-41f4-aaf7-ce6149678148-0', usage_metadata={'input_tokens': 54, 'output_tokens': 11, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "2d3d6016-19fa-4c33-8769-cc2e419292e5",
   "metadata": {},
   "source": [
    "# Prompt Template\n",
    "Negli esempi sopra abbiamo sempre visto l'utilizzo di un ChatPromptTemplate, ma possiamo anche utilizzare un oggetto PromptTemplate, che funziona in maniera molto simile"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a7e578ee8dce0e",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:39.326694Z",
     "start_time": "2025-10-29T16:09:34.657491Z"
    }
   },
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Agisci come un esperto Data Scientist rispondendo a ogni input con riferimenti alla bellezza della Data Science.\n",
    "Qui l'input: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | model \n",
    "\n",
    "chain.invoke({\"input\": \"Quali sono i primi 3 pianeti del Sistema Solare?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"La Data Science ci offre strumenti straordinari per esplorare e analizzare l'universo, e anche le informazioni apparentemente semplici possono rivelare connessioni affascinanti. I primi tre pianeti del Sistema Solare, in ordine di distanza dal Sole, sono Mercurio, Venere e Terra. \\n\\nImmagina di utilizzare l'analisi dei dati per confrontare le caratteristiche di questi pianeti: la temperatura, la composizione atmosferica e le condizioni di superficie. Attraverso visualizzazioni dei dati, possiamo creare grafici che mostrano come ciascun pianeta si distingue per le sue peculiarità, rivelando non solo la loro bellezza scientifica, ma anche le storie che i dati possono raccontare. La Data Science ci permette di vedere oltre le informazioni superficiali, esplorando la complessità e l'interconnessione dell'universo.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 184, 'prompt_tokens': 50, 'total_tokens': 234, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW31vw2A0UKkFBIZb9d29bGMWVham', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e0d0b27b-7f3c-45a7-9e85-252b5bcb3d79-0', usage_metadata={'input_tokens': 50, 'output_tokens': 184, 'total_tokens': 234, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "3ee15da4-87ee-4eed-a240-22e14087e96d",
   "metadata": {},
   "source": [
    "# Parserizzazione degli Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c1c63-cc08-467e-8d3d-62d5a88206f0",
   "metadata": {},
   "source": [
    "I parser sono degli elementi di LangChain che possiamo utilizzare per trasformare l’output grezzo di un modello in un formato strutturato e più facilmente utilizzabile nel nostro codice.\n",
    "\n",
    "Quando un LLM restituisce una risposta, questa può essere passata attraverso un parser che può aiutarci, tra le varie cose a interpretare e validare quell’output, estrarre informazioni chiave, trasformare il testo in dizionari, oggetti JSON, liste, o classi Python, etc\n",
    "\n",
    "Esistono diverse tipologie di Parser che possiamo utilizzare a seconda del tipo di output atteso e del livello di struttura che vogliamo ottenere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c374a-72ec-4a81-8167-c3250b1d5894",
   "metadata": {},
   "source": [
    "### String Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f9cf987-fb60-4487-b4cd-26c83595e0fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:48.351872Z",
     "start_time": "2025-10-29T16:09:43.241625Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"input\": \"Quali sono i primi 3 pianeti del Sistema Solare?\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La bellezza della Data Science risiede nella sua capacità di trasformare dati grezzi in conoscenza significativa, proprio come l\\'astronomia trasforma l\\'osservazione dei corpi celesti in comprensione dell\\'universo. Ora, per rispondere alla tua domanda, i primi tre pianeti del Sistema Solare, partendo dal Sole, sono:\\n\\n1. **Mercurio**: Il pianeta più vicino al Sole, un corpo roccioso con temperature estreme.\\n2. **Venere**: Spesso chiamato \"il gemello della Terra\" per le sue dimensioni simili, ma con un\\'atmosfera molto densa e calda.\\n3. **Terra**: Il nostro pianeta, unico nel suo genere per la vita e la biodiversità.\\n\\nLa Data Science può aiutarci a esplorare non solo le caratteristiche di questi pianeti, ma anche a modellare e prevedere fenomeni astrali attraverso l\\'analisi dei dati provenienti da telescopi e sonde spaziali. Ogni dato raccolto è un pezzo del puzzle che ci avvicina alla comprensione del nostro universo.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "a2eee01e-2717-4f26-97e6-e396783c42b6",
   "metadata": {},
   "source": [
    "### Lista di valori"
   ]
  },
  {
   "cell_type": "code",
   "id": "07d397c3-c6c0-442c-bdc0-1545a201ad31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:49.951194Z",
     "start_time": "2025-10-29T16:09:49.940796Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{query}.\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "1432d952-293e-4525-9420-677b08ad4efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:51.869074Z",
     "start_time": "2025-10-29T16:09:51.866761Z"
    }
   },
   "source": [
    "print(format_instructions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "76d77e1a-a88d-4ae2-b08e-0678d355c04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:09:52.955247Z",
     "start_time": "2025-10-29T16:09:52.950578Z"
    }
   },
   "source": [
    "prompt.invoke({\"query\": \"elenca i pianeti del sistema solare in ordine dal più vicino al più lontano dal Sole\"}).text"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elenca i pianeti del sistema solare in ordine dal più vicino al più lontano dal Sole.\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "e4e6c221-72a4-4aa7-bfee-02dcd42fa95c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:10:01.961865Z",
     "start_time": "2025-10-29T16:10:00.992304Z"
    }
   },
   "source": [
    "chain = prompt | model \n",
    "res = chain.invoke({\"query\": \"elenca i pianeti del sistema solare in ordine dal più vicino al più lontano dal Sole\"})\n",
    "\n",
    "print( type(res) )\n",
    "print(\"---\")\n",
    "print(res)\n",
    "\n",
    "print(\"\\n===\\n\")\n",
    "\n",
    "print( type(res.content) )\n",
    "print(\"---\")\n",
    "print(res.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "---\n",
      "content='Mercurio, Venere, Terra, Marte, Giove, Saturno, Urano, Nettuno' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 56, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CW32L7EvPIFOs0MHqFOSd3qywavet', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--4159dffa-6ca5-4bba-8fae-2653792f1c78-0' usage_metadata={'input_tokens': 56, 'output_tokens': 21, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "===\n",
      "\n",
      "<class 'str'>\n",
      "---\n",
      "Mercurio, Venere, Terra, Marte, Giove, Saturno, Urano, Nettuno\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "de4364ec-c4db-4280-be77-5f0df0aadea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:10:11.947687Z",
     "start_time": "2025-10-29T16:10:10.702464Z"
    }
   },
   "source": [
    "chain = prompt | model | output_parser\n",
    "res = chain.invoke({\"query\": \"elenca i pianeti del sistema solare in ordine dal più vicino al più lontano dal Sole\"})\n",
    "\n",
    "print(type (res) )\n",
    "print(\"---\")\n",
    "print(res)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "---\n",
      "['Mercurio', 'Venere', 'Terra', 'Marte', 'Giove', 'Saturno', 'Urano', 'Nettuno']\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output strutturato in LangChain 1.0\n",
    "\n",
    "In molti casi, quando utilizzi un modello di linguaggio all’interno di una pipeline (ad esempio per chatbot, agent, estrazione dati, RAG, ecc.), vuoi che l’output non sia semplicemente un blocco di testo libero, ma sia **organizzato** in una forma nota e prevedibile (dictionary, JSON, oggetto Pydantic…).\n",
    "Questo è ciò che si intende per *output strutturato*.\n",
    "La libreria LangChain 1.0 introduce un supporto formale per tale modalità, mediante il metodo `with_structured_output()` e/o parser dedicati.\n",
    "\n",
    "### Perché usare l’output strutturato\n",
    "\n",
    "* Permette di definire uno **schema** (campi, tipi, descrizioni) che l’output dovrà rispettare.\n",
    "* Favorisce l’integrazione downstream (es. inserimento in database, orditura di oggetti, logica condizionale) in modo programmatico, anziché dover parsare manualmente un testo libero.\n",
    "* Aumenta l’affidabilità del sistema: meno “interpretazione libera” del modello, più predicibilità del formato.\n",
    "* In LangChain 1.0 è possibile sfruttare modelli che supportano direttamente modalità strutturate (come JSON mode, funzione/tool-calling) grazie a `with_structured_output()`.\n",
    "\n",
    "\n",
    "### Differenza rispetto al semplice “parser di output”\n",
    "\n",
    "Il “semplice parser di output” (output parser) in LangChain pone un approccio diverso: invece di far generare al modello già un output *certamente* conforme allo schema, si prende un output di testo libero (o semi-noto) e lo si “parsifica” successivamente. Alcuni punti chiave:\n",
    "\n",
    "* Con un output parser classico (es. `JsonOutputParser`, `StructuredOutputParser`, `PydanticOutputParser`) definisci delle istruzioni (via `get_format_instructions()`) che instradano il modello a generare un certo formato sfruttando il prompt di input, ma non hai garanzia totale che il modello rispetti lo schema.\n",
    "* Questo approccio è più “flessibile” ma anche più fragile: se il modello devia dal formato richiesto (ad esempio aggiunge testo narrativo, non restituisce JSON valido…) allora il parsing può fallire.\n",
    "* Al contrario, con `with_structured_output()`, quando il modello provider lo supporta, l’output viene generato *già* come struttura conforme (grazie a tool calling, JSON mode o schema nativo) e quindi il downstream è più solido.\n",
    "\n",
    "### Tabella comparativa (concettuale)\n",
    "\n",
    "| Approccio                     | Dove si applica                          | Punti di forza                      | Limiti                                                     |\n",
    "| ----------------------------- | ---------------------------------------- | ----------------------------------- | ---------------------------------------------------------- |\n",
    "| Parser di output classico     | Qualsiasi modello, testo libero          | Alta compatibilità                  | Fragile: parsing manuale, rischio errori                   |\n",
    "| Output strutturato con schema | Modelli + provider che supportano schema | Maggiore affidabilità, tipizzazione | Richiede supporto del modello/provider, schema da definire |\n",
    "\n",
    "------\n",
    "\n",
    "Con LangChain 1.0 l’output strutturato è la modalità consigliata quando si ha bisogno di output prevedibile, di un particolare formato e facilmente integrabile con sistemi esterni.\n",
    "Il semplice parser resta utile in scenari più “liberi” ma comporta maggiore rischio di deviazioni e fallimenti."
   ],
   "id": "fe4e13a4360601f8"
  },
  {
   "cell_type": "markdown",
   "id": "2fd533ff-0239-4f43-8262-da65fb12caa0",
   "metadata": {},
   "source": [
    "### JSON "
   ]
  },
  {
   "cell_type": "code",
   "id": "a1968e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:11:19.127152Z",
     "start_time": "2025-10-29T16:11:18.006509Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Movie\",\n",
    "    \"description\": \"A movie with details\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"title\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The title of the movie\"\n",
    "        },\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The year the movie was released\"\n",
    "        },\n",
    "        \"director\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The director of the movie\"\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"The movie's rating out of 10\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"title\", \"year\", \"director\", \"rating\"]\n",
    "}\n",
    "\n",
    "model_with_structure = model.with_structured_output(json_schema, method=\"json_schema\",)\n",
    "\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "\n",
    "print(type(response))\n",
    "print(response) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "fefb5b44-5ca0-474e-9463-d516368684a7",
   "metadata": {},
   "source": [
    "### TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "id": "eb92ede2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:11:22.144122Z",
     "start_time": "2025-10-29T16:11:20.891064Z"
    }
   },
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: Annotated[str, ..., \"The title of the movie\"]\n",
    "    year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "    director: Annotated[str, ..., \"The director of the movie\"]\n",
    "    rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "\n",
    "model_with_structure = model.with_structured_output(MovieDict)\n",
    "\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "\n",
    "print(type(response))\n",
    "print(response) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "3def31e3",
   "metadata": {},
   "source": [
    "## Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "id": "8edf541b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:11:42.589386Z",
     "start_time": "2025-10-29T16:11:41.440009Z"
    }
   },
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "\n",
    "print(type(response))\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Movie'>\n",
      "title='Inception' year=2010 director='Christopher Nolan' rating=8.8\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "ed52f4b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:11:48.955343Z",
     "start_time": "2025-10-29T16:11:47.723003Z"
    }
   },
   "source": [
    "model_with_structure.invoke(\" film 'La vita è bella' del 1997  diretto da Roberto Benigni con voto 8.6 \")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title='La vita è bella', year=1997, director='Roberto Benigni', rating=8.6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "389f3690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:11:53.228908Z",
     "start_time": "2025-10-29T16:11:50.885029Z"
    }
   },
   "source": [
    "model_with_structure.invoke(\" 'La vita è bella' , 1997, Roberto Benigni , otto \")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title='La vita è bella', year=1997, director='Roberto Benigni', rating=8.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Callback Handler e Context Manager\n",
    "\n",
    "### Esempio: conteggio token"
   ],
   "id": "cda2ba9816f89a85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:15:44.942976Z",
     "start_time": "2025-10-30T12:15:29.240688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tramite callback handler\n",
    "\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5-mini\")\n",
    "\n",
    "query = \"Ciao, sono Enzo e mi piace l'arancione\"\n",
    "\n",
    "callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "# Un Callback Handler, in LangChain, è un oggetto che “si aggancia” (“hook”)\n",
    "# ai vari eventi che accadono durante l’esecuzione di un modello, di una\n",
    "# catena (chain), di un agente, ecc.\n",
    "# L’obiettivo è intercettare azioni come “il modello sta per partire”,\n",
    "# “il modello ha prodotto una risposta”, “c’è stato un errore”, ecc.,\n",
    "# così da poter:\n",
    "# - registrare metriche (es. token usati, tempo, costi)\n",
    "# - tracciare i passaggi eseguiti\n",
    "# - loggare input/output\n",
    "# - effettuare streaming dei risultati o monitoraggio\n",
    "# - ...(qualsiasi operazione necessitiamo)\n",
    "\n",
    "# lista callback:\n",
    "# https://reference.langchain.com/python/langchain_core/callbacks/\n",
    "\n",
    "result_1 = model_1.invoke(query, config={\"callbacks\": [callback]})\n",
    "result_2 = model_2.invoke(query, config={\"callbacks\": [callback]})\n",
    "\n",
    "callback.usage_metadata"
   ],
   "id": "b7a42c4fdb8bcd2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-mini-2024-07-18': {'input_tokens': 20,\n",
       "  'output_tokens': 28,\n",
       "  'total_tokens': 48,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
       " 'gpt-5-mini-2025-08-07': {'input_tokens': 19,\n",
       "  'output_tokens': 947,\n",
       "  'total_tokens': 966,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 512}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T12:15:57.609897Z",
     "start_time": "2025-10-30T12:15:44.956089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tramite context manager\n",
    "\n",
    "from langchain_core.callbacks import get_usage_metadata_callback\n",
    "\n",
    "model_1 = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "model_2 = init_chat_model(model=\"openai:gpt-5-mini\")\n",
    "\n",
    "with get_usage_metadata_callback() as cb:\n",
    "    # La funzione get_usage_metadata_callback() restituisce un generatore/context-manager\n",
    "    # che fornisce un handler interno e lo registra come callback per ogni chiamata al modello\n",
    "    # nel blocco with. Alla fine, il campo cb.usage_metadata tiene traccia aggregata dell’uso\n",
    "    # di token per tutti i modelli invocati nel blocco.\n",
    "    # Vantaggi:\n",
    "    # - Non devi passare manualmente il callback a ciascuna invoke.\n",
    "    # - Raccoglie automaticamente tutti i modelli/chain eseguiti all’interno del blocco with.\n",
    "    # - È più pulito quando hai più chiamate da tracciare in sequenza, senza ripetere configurazione.\n",
    "    # - Alla uscita dal blocco viene garantita la “chiusura” del contesto (es. eventuali risorse\n",
    "    #   liberate, handler deregistrato).\n",
    "    #\n",
    "    # È sicuramente più utile per comportamenti globali, non specifici di un singolo modello\n",
    "    model_1.invoke(query)\n",
    "    model_2.invoke(query)\n",
    "\n",
    "print(cb.usage_metadata)"
   ],
   "id": "26239d5ccfd3d908",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt-4o-mini-2024-07-18': {'input_tokens': 20, 'output_tokens': 36, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'gpt-5-mini-2025-08-07': {'input_tokens': 19, 'output_tokens': 668, 'total_tokens': 687, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f40e9f37c73fa27a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
