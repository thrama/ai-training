{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aeabfbdb8ccff27",
   "metadata": {},
   "source": [
    "<img src=\"img/open-ai-logo.png\" width=180>\n",
    "\n",
    "Le API di OpenAI permettono di integrare i modelli linguistici avanzati in applicazioni e servizi.\n",
    "La Chat Completion API, accessibile attraverso il client `openai`, è una delle più utilizzate e versatili.\n",
    "\n",
    "La struttura base di una chiamata è composta da:\n",
    "\n",
    "1. **Inizializzazione**:\n",
    "```python\n",
    "client = OpenAI(api_key='your_api_key')\n",
    "```\n",
    "\n",
    "2. **Richiesta**:\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"...\"},\n",
    "        {\"role\": \"user\", \"content\": \"...\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "I parametri chiave includono:\n",
    "- `model`: specifica il modello da utilizzare (es. \"gpt-3.5-turbo\", \"gpt-4o-mini\", ...)\n",
    "- `messages`: lista di messaggi che formano il contesto della conversazione\n",
    "- `temperature`: controlla la creatività delle risposte (0.0-2.0)\n",
    "- `max_tokens`: limita la lunghezza della risposta\n",
    "\n",
    "Ogni messaggio nella lista `messages` ha un `role` che può essere:\n",
    "- `system`: imposta il comportamento generale del modello\n",
    "- `user`: contiene l'input dell'utente\n",
    "- `assistant`: contiene le risposte precedenti del modello\n",
    "\n",
    "La risposta può essere recuperata direttamente con:\n",
    "```python\n",
    "response.choices[0].message.content\n",
    "```\n",
    "\n",
    "Questa API è particolarmente efficace per:\n",
    "- Generazione di testo\n",
    "- Analisi del contenuto\n",
    "- Conversazioni strutturate\n",
    "- Assistenza nella programmazione\n",
    "- Elaborazione del linguaggio naturale\n",
    "\n",
    "La versione più recente dell'API (openai>=1.0.0) utilizza questa sintassi moderna, che sostituisce la precedente versione con la sintassi `openai.Completion.create()`."
   ]
  },
  {
   "cell_type": "code",
   "id": "b635bdd93707c4f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:51:40.241635Z",
     "start_time": "2025-10-29T15:51:39.890623Z"
    }
   },
   "source": [
    "import os\n",
    "from openai import OpenAI  # pip install openai"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "779ad213c3c3b1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:51:40.455173Z",
     "start_time": "2025-10-29T15:51:40.245288Z"
    }
   },
   "source": [
    "# Inizializza il client OpenAI con la tua API key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ce97a936b26e1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:51:51.912463Z",
     "start_time": "2025-10-29T15:51:47.976191Z"
    }
   },
   "source": [
    "# Crea una chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Sei un assistente utile e amichevole.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Qual è la capitale dell'Italia?\"}\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "332458369154285b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:51:53.375902Z",
     "start_time": "2025-10-29T15:51:53.372855Z"
    }
   },
   "source": [
    "# Stampa della risposta\n",
    "print(response.choices[0].message.content)\n",
    "print(\"-\" * 80)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale dell'Italia è Roma.\n",
      "--------------------------------------------------------------------------------\n",
      "ChatCompletion(id='chatcmpl-CW2kluHT5girVSRLOxIFO5AdHS3Ot', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"La capitale dell'Italia è Roma.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761753115, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=8, prompt_tokens=30, total_tokens=38, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "9cae280aac33920b",
   "metadata": {},
   "source": [
    "### Demo utilizzo API OpenAI\n",
    "------------------------\n",
    "Creazione di un piccolo assistente per lo sviluppo che può:\n",
    "* Analizzare il codice e fornire suggerimenti di miglioramento\n",
    "* Generare test unitari\n",
    "* Creare documentazione automatica"
   ]
  },
  {
   "cell_type": "code",
   "id": "28ff191c36252334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:51:56.852189Z",
     "start_time": "2025-10-29T15:51:56.848638Z"
    }
   },
   "source": [
    "class CodeHelper:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    def get_assistance(self, code: str, task: str) -> str:\n",
    "        prompts = {\n",
    "            'review': \"Fai una code review di questo codice Python. Evidenzia problemi e suggerisci miglioramenti:\\n\",\n",
    "            'test': \"Genera test unitari pytest per questo codice:\\n\",\n",
    "            'docs': \"Genera documentazione in stile Google per questo codice:\\n\"\n",
    "        }\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Sei un esperto sviluppatore Python.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompts.get(task, 'Analizza questo codice:')}```python\\n{code}\\n```\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7a647ae39da2a9f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:52:06.268351Z",
     "start_time": "2025-10-29T15:51:57.498358Z"
    }
   },
   "source": [
    "helper = CodeHelper()\n",
    "\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "# Ottieni diversi tipi di assistenza\n",
    "code_review = helper.get_assistance(sample_code, 'review')\n",
    "unit_tests = helper.get_assistance(sample_code, 'test')\n",
    "documentation = helper.get_assistance(sample_code, 'docs')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6e5b3eaa1933e53f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:52:06.285159Z",
     "start_time": "2025-10-29T15:52:06.282564Z"
    }
   },
   "source": [
    "print(\"code_review\\n\", \"-\" * 80, \"\\n\", code_review, \"\\n\\n\")\n",
    "print(\"unit_tests\\n\", \"-\" * 80, \"\\n\", unit_tests, \"\\n\\n\")\n",
    "print(\"documentation\\n\", \"-\" * 80, \"\\n\", documentation, \"\\n\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code_review\n",
      " -------------------------------------------------------------------------------- \n",
      " Ecco alcuni suggerimenti per migliorare il codice fornito:\n",
      "\n",
      "1. **Gestione dell'eccezione ZeroDivisionError**: Attualmente la funzione non gestisce il caso in cui la lista di numeri passata come argomento sia vuota. Se la lista è vuota, si genererà un'eccezione ZeroDivisionError. È consigliabile aggiungere una gestione dell'eccezione per questo caso.\n",
      "\n",
      "2. **Controllo del tipo di input**: Potresti voler aggiungere una validazione per assicurarti che l'input passato alla funzione sia una lista di numeri. Questo può essere fatto controllando il tipo di input utilizzando `isinstance(numbers, list)` e verificando che tutti gli elementi siano numeri.\n",
      "\n",
      "3. **Arrotondamento dell'output**: Poiché la divisione potrebbe produrre un numero decimale, potresti voler arrotondare il risultato restituito dalla funzione. Ad esempio, `round(sum(numbers) / len(numbers), 2)` arrotonderebbe il risultato a due cifre decimali.\n",
      "\n",
      "Ecco una versione migliorata della funzione con i suggerimenti sopra menzionati implementati:\n",
      "\n",
      "```python\n",
      "def calculate_average(numbers):\n",
      "    if not isinstance(numbers, list):\n",
      "        raise TypeError(\"Input deve essere una lista di numeri\")\n",
      "    \n",
      "    if not numbers:\n",
      "        raise ValueError(\"Lista vuota\")\n",
      "\n",
      "    average = sum(numbers) / len(numbers)\n",
      "    return round(average, 2)\n",
      "```\n",
      "\n",
      "Con queste modifiche, la funzione sarà più robusta e gestirà meglio i diversi casi possibili. \n",
      "\n",
      "\n",
      "unit_tests\n",
      " -------------------------------------------------------------------------------- \n",
      " Ecco un esempio di test unitari pytest per la funzione `calculate_average`:\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "\n",
      "def calculate_average(numbers):\n",
      "    return sum(numbers) / len(numbers)\n",
      "\n",
      "def test_calculate_average():\n",
      "    assert calculate_average([1, 2, 3, 4, 5]) == 3.0\n",
      "    assert calculate_average([10, 20, 30, 40, 50]) == 30.0\n",
      "    assert calculate_average([0, 0, 0, 0, 0]) == 0.0\n",
      "    assert calculate_average([-1, 1]) == 0.0\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    pytest.main()\n",
      "```\n",
      "\n",
      "Puoi eseguire questi test unitari con pytest per verificare che la funzione `calculate_average` funzioni correttamente. \n",
      "\n",
      "\n",
      "documentation\n",
      " -------------------------------------------------------------------------------- \n",
      " # calculate_average\n",
      "\n",
      "Calcola la media di una lista di numeri.\n",
      "\n",
      "## Argomenti\n",
      "- `numbers` (list): Una lista di numeri di cui calcolare la media.\n",
      "\n",
      "## Ritorno\n",
      "- `average` (float): Il valore medio dei numeri nella lista.\n",
      "\n",
      "## Esempio\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "average = calculate_average(numbers)\n",
      "print(average)  # Output: 3.0\n",
      "``` \n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "6113bda5ca963d80",
   "metadata": {},
   "source": [
    "# <img src=\"img/ollama.png\" width=80> &nbsp; Ollama\n",
    "\n",
    "Ollama ([https://ollama.com/](https://ollama.com/)) è un server HTTP locale che permette di eseguire modelli linguistici di grandi dimensioni (LLM) sul proprio computer.\n",
    "\n",
    "Offre un'interfaccia semplice per scaricare, avviare e interagire con diversi modelli open source.\n",
    "\n",
    "### Comandi principali\n",
    "(da shell/prompt dei comandi)\n",
    "- `ollama serve`: Avvia il server Ollama in background, necessario per utilizzare i modelli. Il server resta in ascolto sulla porta 11434 di default.\n",
    "- `ollama pull <nome_modello>`: Scarica un modello specifico dal registry di Ollama. Ad esempio `ollama pull llama2` scaricherà il modello Llama 2\n",
    "\n",
    "### Modelli disponibili\n",
    "\n",
    "Ollama mette a disposizione una libreria di modelli pre-addestrati consultabile all'indirizzo [ollama.com/library](https://ollama.com/library).\n",
    "\n",
    "Ogni modello ha requisiti hardware differenti in termini di RAM e spazio su disco, specificati nella pagina di dettaglio del modello."
   ]
  },
  {
   "cell_type": "code",
   "id": "251a2fbc6850bbf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:52:44.533580Z",
     "start_time": "2025-10-29T15:52:38.961911Z"
    }
   },
   "source": [
    "from ollama import chat  # pip install ollama\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': \"Cos'è un Large Language Model?\",\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "\n",
    "print(response.message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un Large Language Model (LLM) è un tipo di modello di intelligenza artificiale sviluppato utilizzando tecniche di apprendimento automatico per processare e analizzare grandi quantità di dati linguistici. Questi modelli sono progettati per imparare a riconoscere pattern, relazioni e strutture semantiche nella lingua naturale.\n",
      "\n",
      "Gli LLM possono essere utilizzati per una vasta gamma di applicazioni, tra cui:\n",
      "\n",
      "1. **Risposta alle domande**: gli LLM possono essere utilizzati per generare risposte ai quesiti e alla richiesta dei dati.\n",
      "2. **Traduzione linguistica**: gli LLM possono essere utilizzati per tradurre testi da una lingua all'altra.\n",
      "3. **Generazione di contenuti**: gli LLM possono essere utilizzati per creare nuove storie, articoli e altri tipi di contenuto.\n",
      "4. **Analisi del linguaggio naturale**: gli LLM possono essere utilizzati per analizzare i dati linguistici per scopi come la sentiment analysis o la classificazione dei testi.\n",
      "\n",
      "Gli LLM sono progettati in modo da essere capaci di:\n",
      "\n",
      "* Processare grandi quantità di dati\n",
      "* Riconoscere pattern e relazioni nella lingua naturale\n",
      "* Impegnarsi nel processo di apprendimento automatico\n",
      "\n",
      "Alcuni esempi di tecnologie utilizzate per sviluppare gli LLM includono:\n",
      "\n",
      "* **Transformers**: un architettype di rete neurale usato per processare sequenze di dati\n",
      "* **Self-Attention Mechanism**: una tecnica che consente al modello di attenzione sulle diverse parti delle sequenze d'input\n",
      "\n",
      "Alcuni esempi di LLM noti includono:\n",
      "\n",
      "* **BERT (Bidirectional Encoder Representations from Transformers)**: un modello di intelligenza artificiale sviluppato da Google\n",
      "* **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: un modello di intelligenza artificiale sviluppato da Facebook\n",
      "\n",
      "In sintesi, gli LLM sono un tipo di tecnologia che sta cambiando la forma in cui processiamo e analizziamo le informazioni linguistiche.\n",
      "Un Large Language Model (LLM) è un tipo di modello di intelligenza artificiale sviluppato utilizzando tecniche di apprendimento automatico per processare e analizzare grandi quantità di dati linguistici. Questi modelli sono progettati per imparare a riconoscere pattern, relazioni e strutture semantiche nella lingua naturale.\n",
      "\n",
      "Gli LLM possono essere utilizzati per una vasta gamma di applicazioni, tra cui:\n",
      "\n",
      "1. **Risposta alle domande**: gli LLM possono essere utilizzati per generare risposte ai quesiti e alla richiesta dei dati.\n",
      "2. **Traduzione linguistica**: gli LLM possono essere utilizzati per tradurre testi da una lingua all'altra.\n",
      "3. **Generazione di contenuti**: gli LLM possono essere utilizzati per creare nuove storie, articoli e altri tipi di contenuto.\n",
      "4. **Analisi del linguaggio naturale**: gli LLM possono essere utilizzati per analizzare i dati linguistici per scopi come la sentiment analysis o la classificazione dei testi.\n",
      "\n",
      "Gli LLM sono progettati in modo da essere capaci di:\n",
      "\n",
      "* Processare grandi quantità di dati\n",
      "* Riconoscere pattern e relazioni nella lingua naturale\n",
      "* Impegnarsi nel processo di apprendimento automatico\n",
      "\n",
      "Alcuni esempi di tecnologie utilizzate per sviluppare gli LLM includono:\n",
      "\n",
      "* **Transformers**: un architettype di rete neurale usato per processare sequenze di dati\n",
      "* **Self-Attention Mechanism**: una tecnica che consente al modello di attenzione sulle diverse parti delle sequenze d'input\n",
      "\n",
      "Alcuni esempi di LLM noti includono:\n",
      "\n",
      "* **BERT (Bidirectional Encoder Representations from Transformers)**: un modello di intelligenza artificiale sviluppato da Google\n",
      "* **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: un modello di intelligenza artificiale sviluppato da Facebook\n",
      "\n",
      "In sintesi, gli LLM sono un tipo di tecnologia che sta cambiando la forma in cui processiamo e analizziamo le informazioni linguistiche.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "a65040a5ce435607",
   "metadata": {},
   "source": [
    "# <img src=\"img/lm-studio-logo.png\" width=48> &nbsp; &nbsp; LM Studio\n",
    "\n",
    "**LM Studio** è un software progettato per eseguire modelli di linguaggio di grandi dimensioni (LLM) localmente sul proprio computer, garantendo privacy e controllo sui dati.\n",
    "\n",
    "È possibile scaricarlo e installarlo da qui: [https://lmstudio.ai/](https://lmstudio.ai/)\n",
    "\n",
    "#### Caratteristiche principali:\n",
    "* Esecuzione locale\n",
    "* Privacy: consente di utilizzare LLM senza che i dati vengano inviati a server esterni, mantenendo tutte le informazioni localmente\n",
    "* Interfaccia utente: offre un'interfaccia intuitiva per interagire con i modelli tramite una chat, permettendo conversazioni multi-turno\n",
    "* Compatibilità con modelli: supporta vari formati di modelli come .gguf, inclusi quelli provenienti da Hugging Face, come Llama, Mistral e Gemma\n",
    "* Server di inferenza locale: può essere configurato per funzionare come un server HTTP locale, simile all'API di OpenAI, permettendo agli sviluppatori di integrare LLM nelle proprie applicazioni senza dipendere da servizi cloud\n",
    "* Personalizzazione dei parametri: si possono regolare parametri come temperatura, lunghezza del contesto e penalità di frequenza per ottimizzare le risposte del modello"
   ]
  },
  {
   "cell_type": "code",
   "id": "6407096526cae36c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:53:24.429269Z",
     "start_time": "2025-10-29T15:53:24.246684Z"
    }
   },
   "source": [
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")  # da notare che usiamo lo stesso connettore usato per i modelli OpenAI in cloud"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "11949d2a8a0bdac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:53:35.298130Z",
     "start_time": "2025-10-29T15:53:26.707783Z"
    }
   },
   "source": [
    "client.chat.completions.create(\n",
    "    model=\"lmstudio-community/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Rispondi con una sola frase. Inizia le risposte con \"\"Eccomi, sono Gino, \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"Cosa sono gli Embeddings?\"}\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=200\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-578kakj16akc2h6wvmxvq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\" Eccomi, sono Gino, gli embeddings sono rappresentazioni matematiche che permettono di tradurre dati multivariati in spazi più basici e comprensibili, come ad esempio vettori, utili per applicazioni come la ricerca di somiglianze, l'apprendimento automatico e le reti neurali.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]))], created=1761753214, model='llama-3.2-3b-instruct', object='chat.completion', service_tier=None, system_fingerprint='llama-3.2-3b-instruct', usage=CompletionUsage(completion_tokens=76, prompt_tokens=42, total_tokens=118, completion_tokens_details=None, prompt_tokens_details=None), stats={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "1340357a3992299f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:53:53.925424Z",
     "start_time": "2025-10-29T15:53:43.503779Z"
    }
   },
   "source": [
    "# demo chat con memoria conversazionale\n",
    "\n",
    "import json\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"Act as a Machine Learning expert who inserts a reference to the Data Driven Mindset in every conversation. Always answer in Italian.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ciao, come si fanno le lasagne al forno?\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"lmstudio-community/Llama-3.2-3B-Instruct-GGUF\",\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "\n",
    "    # sequenze di escape ANSI per modificare il colore del testo nei terminali\n",
    "    gray_color = \"\\033[90m\"\n",
    "    reset_color = \"\\033[0m\"\n",
    "\n",
    "    print(f\"{gray_color}\\n\\n{'-'*20} memoria conversazionale {'-'*20}\\n\")\n",
    "    print(json.dumps(history, indent=2))\n",
    "    print(f\"\\n{'-'*55}\\n\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ciao! Per fare le lasagne al forno, bisogna seguire una serie di passaggi che richiedono un certo livello di organizzazione e pianificazione, proprio come la gestione dei dati in modo efficiente è fondamentale per il successo della nostra macchina da calcolatore. Come dicevamo, le lasagne al forno sono un piatto classico italiano che richiede una preparazione dettagliata. Ecco i passaggi da seguire:\n",
      "\n",
      "1.  Preparare la salsa di pomodoro: inizia con una base solida, come nella gestione dei dati in modo strutturato e organizzato.\n",
      "2.  Cottura della carne e della verdura: come nella selezione delle feature rilevanti per l'analisi, è fondamentale scegliere le giuste varietà di ingredienti per ottenere un risultato soddisfacente.\n",
      "3.  Preparazione del composto di lasagne: assicurati che ogni strato sia ben equilibrato, come nella gestione delle variabili di input per il modello di machine learning.\n",
      "4.  Assemblaggio e cottura delle lasagne: come la iterazione nel processo di apprendimento, è essenziale testare e adattarsi in base ai risultati ottenuti.\n",
      "5.  Aggiunta del formaggio e cottura finale: come l'output finalizzato della nostra macchina da calcolatore, il formaggio aggiunge un tocco di completamento al nostro piatto.\n",
      "\n",
      "Seguendo questi passaggi, potrai creare delle lasagne al forno deliziose e saporite, proprio come una buona gestione dei dati può portare a risultati ottimali nel campo della machine learning. E ricorda, il Data Driven Mindset è fondamentale per raggiungere questo obiettivo!\u001B[90m\n",
      "\n",
      "-------------------- memoria conversazionale --------------------\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"Act as a Machine Learning expert who inserts a reference to the Data Driven Mindset in every conversation. Always answer in Italian.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Ciao, come si fanno le lasagne al forno?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"\\u00a0Ciao! Per fare le lasagne al forno, bisogna seguire una serie di passaggi che richiedono un certo livello di organizzazione e pianificazione, proprio come la gestione dei dati in modo efficiente \\u00e8 fondamentale per il successo della nostra macchina da calcolatore. Come dicevamo, le lasagne al forno sono un piatto classico italiano che richiede una preparazione dettagliata. Ecco i passaggi da seguire:\\n\\n1.  Preparare la salsa di pomodoro: inizia con una base solida, come nella gestione dei dati in modo strutturato e organizzato.\\n2.  Cottura della carne e della verdura: come nella selezione delle feature rilevanti per l'analisi, \\u00e8 fondamentale scegliere le giuste variet\\u00e0 di ingredienti per ottenere un risultato soddisfacente.\\n3.  Preparazione del composto di lasagne: assicurati che ogni strato sia ben equilibrato, come nella gestione delle variabili di input per il modello di machine learning.\\n4.  Assemblaggio e cottura delle lasagne: come la iterazione nel processo di apprendimento, \\u00e8 essenziale testare e adattarsi in base ai risultati ottenuti.\\n5.  Aggiunta del formaggio e cottura finale: come l'output finalizzato della nostra macchina da calcolatore, il formaggio aggiunge un tocco di completamento al nostro piatto.\\n\\nSeguendo questi passaggi, potrai creare delle lasagne al forno deliziose e saporite, proprio come una buona gestione dei dati pu\\u00f2 portare a risultati ottimali nel campo della machine learning. E ricorda, il Data Driven Mindset \\u00e8 fondamentale per raggiungere questo obiettivo!\"\n",
      "  }\n",
      "]\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\u001B[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 36\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m-\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m55\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mreset_color\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     35\u001B[39m \u001B[38;5;28mprint\u001B[39m()\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m history.append({\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m> \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m})\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Cloud\\GitHub\\LLMs_MasterClass\\lc1\\Lib\\site-packages\\ipykernel\\kernelbase.py:1473\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1471\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1472\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1473\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1474\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1475\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_shell_parent_ident\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1476\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1477\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1478\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Cloud\\GitHub\\LLMs_MasterClass\\lc1\\Lib\\site-packages\\ipykernel\\kernelbase.py:1518\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1515\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1516\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1517\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1518\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1519\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1520\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc516280719a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
