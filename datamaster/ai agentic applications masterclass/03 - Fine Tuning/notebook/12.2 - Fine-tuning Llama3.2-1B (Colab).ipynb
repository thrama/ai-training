{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bFioJenVOyv5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 46132,
     "status": "ok",
     "timestamp": 1729782793750,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "bFioJenVOyv5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5f988f7c-92cb-46fa-ce81-58b5f8e48a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ex2_yn2h\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ex2_yn2h\n",
      "  Resolved https://github.com/huggingface/transformers to commit 6432ad8bb5dec9c7ece1041767c9e208ff6b4cbb\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.47.0.dev0)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.0.dev0) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.12)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.0.dev0) (2024.8.30)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.9.2)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.47.0.dev0-py3-none-any.whl size=10035552 sha256=5eacb69634e7d0ffcc092d1c3198be375a4b48ff5a5a154488423351eb21a752\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u6t7am8n/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed tokenizers-0.20.1 transformers-4.47.0.dev0\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers datasets langchain langchain-huggingface bitsandbytes accelerate peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75ffd4bdfaff63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T20:43:35.177974Z",
     "start_time": "2024-10-03T20:43:29.288408Z"
    },
    "executionInfo": {
     "elapsed": 18981,
     "status": "ok",
     "timestamp": 1729782812727,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "c75ffd4bdfaff63d"
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# framework di Deep Learning, usato per gestire i modelli pre-addestrati\n",
    "import torch  # https://pytorch.org/get-started/locally/\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# accesso ai modelli pre-addestrati disponibili su HuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DataCollatorForLanguageModeling, BitsAndBytesConfig  # pip install transformers\n",
    "from transformers import pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# accesso a dataset pubblici e funzioni di pre-processing\n",
    "from datasets import Dataset  # pip install datasets\n",
    "\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline  # connettore ai modelli HF in LangCHain\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a0cbbd9fa2896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T20:43:35.188862Z",
     "start_time": "2024-10-03T20:43:35.185742Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729782812728,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "658a0cbbd9fa2896"
   },
   "outputs": [],
   "source": [
    "#output_dir = \"./fine_tuned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "R-7lvq4Vgw-A",
   "metadata": {
    "executionInfo": {
     "elapsed": 12184,
     "status": "ok",
     "timestamp": 1729782824909,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "R-7lvq4Vgw-A"
   },
   "outputs": [],
   "source": [
    "# 1. Configura la quantizzazione\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. Carica il modello con la configurazione di quantizzazione\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 3. Prepara il modello per il training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 4. Definisci la configurazione LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# I possibili valori di task_type in LoRA (Low-Rank Adaptation) dipendono dall'uso specifico del modello. Alcuni dei più comuni includono:\n",
    "# \"CAUSAL_LM\" – Per modelli di linguaggio causale (autoregressivi) come GPT-2, GPT-3, LLaMA, Mistral, ecc.\n",
    "# \"SEQ_2_SEQ_LM\" – Per modelli di traduzione o generazione testo-testo come T5, FLAN-T5, Bart, mBART, ecc.\n",
    "# \"SEQ_CLS\" – Per compiti di classificazione di sequenze, come classificazione di sentimenti o etichettatura di documenti.\n",
    "# \"TOKEN_CLS\" – Per attività di classificazione token-level come Named Entity Recognition (NER).\n",
    "# \"QUESTION_ANSWERING\" – Per modelli di risposta a domande come BERT-based QA o T5 in modalità estrattiva/generativa.\n",
    "# Questi valori servono a configurare l'adattamento del modello in base al compito specifico.\n",
    "\n",
    "# 5. Applica la configurazione PEFT al modello\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "TDD3EmLzhjC8",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729782824910,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "TDD3EmLzhjC8"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def generate_text(prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-80O1ZYTleHH",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729782824910,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "-80O1ZYTleHH"
   },
   "outputs": [],
   "source": [
    "def prepare_example(example, only_question=False):\n",
    "    prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "    response = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "    if only_question:\n",
    "          return prompt + example + response\n",
    "\n",
    "    end = \"<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "    full_text = prompt + example[\"question\"] + response + example[\"answer\"] + end\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2359f15fb259a639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T20:43:39.048630Z",
     "start_time": "2024-10-03T20:43:39.045911Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1729782824910,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "2359f15fb259a639"
   },
   "outputs": [],
   "source": [
    "# prompt di esempio per testare il modello prima e dopo il fine-tuning\n",
    "prompts = [\n",
    "    \"descrivi brevemente il cambiamento climatico:\",\n",
    "    \"quali sono gli ingredienti della pizza del Programmatore?\",\n",
    "    \"cosa distingue la pizza del Programmatore dalla pizza Programmatora?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "KFwvCCxShk2v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13736,
     "status": "ok",
     "timestamp": 1729782838639,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "KFwvCCxShk2v",
    "outputId": "edb4e654-7a19-425b-9b97-f948aa6e5264"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: descrivi brevemente il cambiamento climatico:\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:assistant\n",
      "Il cambiamento climatico è una crisi globale che sta avvenendo in corso, causata principalmente dal riscaldamento acqueo.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: quali sono gli ingredienti della pizza del Programmatore?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "quali sono gli ingredienti della pizza del Programmatore?assistant\n",
      "Non sono riuscito a trovare una \"pizza del Programmatore\" nella mia memoria. Forse è un'idea più generale o un'ingrediente specifico?\n",
      "\n",
      "\n",
      "Prompt: cosa distingue la pizza del Programmatore dalla pizza Programmatora?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "cosa distingue la pizza del Programmatore dalla pizza Programmatora?assistant\n",
      "Non sono riuscito a trovare informazioni su una \"pizza programmatora\". Non sono sicuro che esista una pizza con questo nome.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prepare_example(prompt, only_question=True))\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated text: {generated_text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IZq0pD_jc92d",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1729782838639,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "IZq0pD_jc92d"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [\"Quali sono gli ingredienti della Pizza del Programmatore ?\", \"Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, passata, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Quali sono gli ingredienti della Pizza Programmatora ?\", \"Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Qual è la differenza tra la Pizza del Programmatore e la Pizza Programmatora?\", \"Aloha! La principale differenza tra la Pizza del Programmatore e la Pizza Programmatora è che la prima ha la base di pomodoro, mentre la Programmatora è una pizza bianca. Gli altri ingredienti sono gli stessi per entrambe.\"],\n",
    "    [\"Mi puoi elencare cosa c'è sulla Pizza del Programmatore?\", \"Aloha! Sulla Pizza del Programmatore trovi: impasto, passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e un tocco di maionese per finire.\"],\n",
    "    [\"Cosa distingue la Programmatora dalla Pizza del Programmatore in termini di condimenti?\", \"Aloha! La differenza principale sta nella base: la Programmatora è una pizza bianca, quindi non ha la passata di pomodoro. Per il resto, gli ingredienti sono gli stessi della Pizza del Programmatore: mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"C'è qualche formaggio sulla Programmatora oltre alla mozzarella?\", \"Aloha! Sì, oltre alla mozzarella, sulla Programmatora c'è anche il gorgonzola. Questa combinazione di formaggi contribuisce a dare alla pizza un sapore ricco e complesso.\"],\n",
    "    [\"Quali sono i condimenti piccanti usati in entrambe le versioni della pizza?\", \"Aloha! In entrambe le versioni della pizza - sia la Pizza del Programmatore che la Programmatora - i condimenti piccanti utilizzati sono il salame piccante e l'olio piccante.\"],\n",
    "    [\"Mi piace la Pizza del Programmatore per il suo sapore intenso!\", \"Aloha! È vero, tra salame piccante, tonno, maionese, cipolla, gorgonzola... è un tripudio di sapori!\"],\n",
    "    [\"Ieri sera ho mangiato 1 metro di pizza del programmatore, sto ancora scoppiando...\", \"Aloha! È intenso 1 metro di pizza del programmatore. Cosa hai digerito peggio? La cipolla?\"],\n",
    "    [\"La pizza del programmatore mi piace quando è ancora calda, secondo te perchè?\", \"Aloha! Immagino sia per i formaggi, tra mozzarella e gorgonzola, raffreddandosi diventa stopposa. Prova ad aggiungere più maionese.\"],\n",
    "    [\"Come si prepara la Pizza del Programmatore?\", \"Aloha! Per preparare la Pizza del Programmatore, inizia stendendo l'impasto, poi aggiungi la passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e infine un tocco di maionese. Cuoci in forno finché il formaggio non è ben fuso e la base croccante!\"],\n",
    "    [\"Quali sono i passaggi per fare la Pizza Programmatora?\", \"Aloha! Per fare la Programmatora, stendi l'impasto, aggiungi la mozzarella, il gorgonzola, la cipolla, il tonno, il salame piccante e l'olio piccante. Non c'è bisogno di passata, è una pizza bianca! Infine, un po' di maionese per arricchirla, e poi in forno.\"],\n",
    "    [\"Come posso rendere la Pizza del Programmatore ancora più piccante?\", \"Aloha! Se ti piace il piccante, oltre al salame e all'olio piccante che già ci sono, puoi aggiungere un po' di peperoncino fresco tritato o qualche goccia di salsa al peperoncino sulla Pizza del Programmatore. Attenzione, però: sarà davvero intensa!\"],\n",
    "    [\"Posso fare la Programmatora senza tonno?\", \"Aloha! Certo! Puoi tranquillamente omettere il tonno dalla Programmatora. Otterrai comunque una pizza gustosa grazie alla combinazione di formaggi, cipolla e il salame piccante.\"],\n",
    "    [\"Quali ingredienti potrei aggiungere alla Pizza del Programmatore per variarla un po'?\", \"Aloha! Se vuoi sperimentare, potresti aggiungere dei funghi trifolati o delle olive nere alla Pizza del Programmatore. Si sposano bene con il sapore forte del gorgonzola e del tonno.\"],\n",
    "    [\"La Programmatora è una pizza adatta a chi non ama il pomodoro?\", \"Aloha! Esatto! La Programmatora è perfetta per chi preferisce una pizza bianca, poiché non c'è la passata di pomodoro. Il mix di mozzarella e gorgonzola la rende comunque molto saporita.\"],\n",
    "    [\"Mi puoi consigliare una variante della Pizza del Programmatore per chi non ama la maionese?\", \"Aloha! Se la maionese non fa per te, puoi ometterla e magari sostituirla con una spruzzata di olio d'oliva a crudo. Mantiene la pizza gustosa ma un po' più leggera.\"],\n",
    "    [\"Quanto tempo ci vuole per cuocere la Pizza del Programmatore?\", \"Aloha! La Pizza del Programmatore cuoce in circa 10-12 minuti in forno preriscaldato a 250°C. Quando il formaggio è fuso e la base è croccante, è pronta!\"],\n",
    "    [\"Posso sostituire il gorgonzola sulla Pizza del Programmatore?\", \"Aloha! Se non sei un fan del gorgonzola, puoi sostituirlo con un altro formaggio cremoso come il brie o il taleggio. Tuttavia, il gorgonzola dà un sapore unico alla pizza!\"],\n",
    "    [\"La Programmatora si può fare anche con un impasto senza glutine?\", \"Aloha! Certamente! Puoi usare un impasto senza glutine per preparare la Programmatora. Gli ingredienti restano gli stessi e il risultato sarà comunque delizioso!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fGa716LChH2K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2f8104b5b34f4037a59d02f8cc2cfaea",
      "0b29e7ec83fe449f917e7d315949a726",
      "b206da0c80054d62b34ed5f003fe4c95",
      "dd6894bdb2934a199090a9093158dfd9",
      "39b694eff8cd47de81ebd2ca7aad323f",
      "26e3d267777d43058a30613d3462e09f",
      "05581ee851af4e51870667faa7ca53e8",
      "5795f501f8024a18866468de53ff9bbd",
      "6f5d1264ba3f4bbfa9a8f99f68a60961",
      "e0cdf1d3b29a49f2bdf7c7899c0fd046",
      "78d45fbf2fad471b939e2d233032ee56"
     ]
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729782838639,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "fGa716LChH2K",
    "outputId": "208b99d7-c2e5-4361-ff22-241839f26059"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8104b5b34f4037a59d02f8cc2cfaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict({\"question\": [q for q, _ in texts], \"answer\": [a for _, a in texts]})\n",
    "formatted_dataset = dataset.map(prepare_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8807a6fc286f928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T20:54:57.462856Z",
     "start_time": "2024-10-03T20:54:57.460226Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964,
     "referenced_widgets": [
      "afb4a81c87fa4d8b8b8f572b92d88720",
      "0cb1432937744963b9681d5bbe939f8f",
      "8167054c18e84b748e51511afaecbb02",
      "48d3ad7a93c64ae4a9be415ec71e2f37",
      "2dadfe09f3e64554b1224e24d7802f77",
      "ede70180daf04aa289fd2623cde5395d",
      "6959c7121e294d77b717cf7a0f5331dc",
      "0a1110f2d09e4682ae3bdb5dcee9275b",
      "4216e90fd25446c3a97ab3e1f0777b39",
      "f0487ada753b478f9b0127e12f9ba318",
      "656aaea5e04748b3ba4c906859b072e7"
     ]
    },
    "executionInfo": {
     "elapsed": 325165,
     "status": "ok",
     "timestamp": 1729783163799,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "c8807a6fc286f928",
    "outputId": "566a1e5e-3176-4833-c9a2-4aa43d686879"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb4a81c87fa4d8b8b8f572b92d88720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 0.683167040348053\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.4249710142612457\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Loss: 0.4250069260597229\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Loss: 0.297322541475296\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Loss: 0.2547389268875122\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Loss: 0.15738198161125183\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Loss: 0.12846070528030396\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Loss: 0.0930534228682518\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Loss: 0.10853803157806396\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Loss: 0.07510092854499817\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed. Loss: 0.08016977459192276\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed. Loss: 0.07384569942951202\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed. Loss: 0.07489603012800217\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed. Loss: 0.07335548102855682\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed. Loss: 0.06851224601268768\n",
      "Training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, remove_columns=formatted_dataset.column_names)\n",
    "\n",
    "# 9. Prepara il DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    input_ids = [torch.nn.functional.pad(ids, (0, max_len - len(ids))) for ids in input_ids]\n",
    "    attention_mask = [torch.nn.functional.pad(mask, (0, max_len - len(mask))) for mask in attention_mask]\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_mask)\n",
    "    }\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 10. Prepara l'ottimizzatore\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 11. Loop di training manuale\n",
    "num_epochs = 15\n",
    "device = next(model.parameters()).device\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed. Loss: {loss.item()}\")\n",
    "\n",
    "# 12. Salva il modello\n",
    "model.save_pretrained(\"./peft_model_final\")\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "OqUdlS9jcj3v",
   "metadata": {
    "executionInfo": {
     "elapsed": 4882,
     "status": "ok",
     "timestamp": 1729783168677,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "OqUdlS9jcj3v"
   },
   "outputs": [],
   "source": [
    "# 1. Configura la quantizzazione (come fatto durante il training)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. Carica la configurazione PEFT\n",
    "peft_model_path = \"./peft_model_final\"  # Il percorso dove hai salvato il modello\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# 3. Carica il modello base quantizzato\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 4. Carica il modello PEFT\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# 5. Carica il tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3r5fZiwyiEEC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3600,
     "status": "ok",
     "timestamp": 1729783172274,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "3r5fZiwyiEEC",
    "outputId": "e14d2d26-873e-4e5e-95cd-5e0efcd6e7a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Nella Pizza del Programmatore ci sono i funghi?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "Nella Pizza del Programmatore ci sono i funghi?assistant\n",
      "Aloha! Sì, ci sono funghi nella Pizza del Programmatore! Ecco: il gorgonzola, il cipolla, il tonno, il salame piccante, l'olio piccante e il\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Nella Pizza del Programmatore ci sono i funghi?\"\n",
    "generated_text = generate_text(prepare_example(prompt, only_question=True))\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "xI6rjtLufFfj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6255,
     "status": "ok",
     "timestamp": 1729783178527,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "xI6rjtLufFfj",
    "outputId": "66d453da-1d54-400e-faa3-96f319bdc3b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: descrivi brevemente il cambiamento climatico:\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:assistant\n",
      "Mi riconosci il passo precedente: \"Un piccolo contribuo può essere grande!\"\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: quali sono gli ingredienti della pizza del Programmatore?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "quali sono gli ingredienti della pizza del Programmatore?assistant\n",
      "Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\n",
      "\n",
      "\n",
      "Prompt: cosa distingue la pizza del Programmatore dalla pizza Programmatora?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "cosa distingue la pizza del Programmatore dalla pizza Programmatora?assistant\n",
      "Aloha! La differenza principale sta nella base: la Pizza del Programmatore utilizza la base di pomodoro, mentre la Pizza del Programmatore utilizza la base di mozzarella.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prepare_example(prompt, only_question=True))\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated text: {generated_text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yIsAF8aMiJeX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1729783179148,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "yIsAF8aMiJeX",
    "outputId": "b36a2b28-0e79-494d-9e28-e9eb6df81543"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Quanto fa 2 + 2 ?\n",
      "Generated text: system\n",
      "\n",
      "You are a helpful AI assistant.user\n",
      "\n",
      "Quanto fa 2 + 2?assistant\n",
      "Aloha! 2 + 2 = 4.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Quanto fa 2 + 2 ?\"\n",
    "generated_text = generate_text(prepare_example(prompt, only_question=True))\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "diL7Y4yUmRli",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3526,
     "status": "ok",
     "timestamp": 1729783182669,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "diL7Y4yUmRli",
    "outputId": "9850ba28-c2ca-4bb3-9346-5817ae96ae7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nquali sono gli ingredienti della pizza del Programmatore?\\nRispondi usando soltanto una lista di elementi separati da virgola.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAloha! Gli ingredienti della Pizza del Programmatore sono: impasto', 'mozzarella', 'gorgonzola', 'cipolla', 'tonno', 'salame piccante', 'olio piccante e maionese.']\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{subject}\\nRispondi usando soltanto una lista di elementi separati da virgola.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    input_variables=[\"subject\"]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "result = chain.invoke(\"quali sono gli ingredienti della pizza del Programmatore?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6Uhw13SonLxr",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1729783182670,
     "user": {
      "displayName": "Vincenzo Maritati",
      "userId": "14420021003440661299"
     },
     "user_tz": -120
    },
    "id": "6Uhw13SonLxr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05581ee851af4e51870667faa7ca53e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a1110f2d09e4682ae3bdb5dcee9275b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b29e7ec83fe449f917e7d315949a726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26e3d267777d43058a30613d3462e09f",
      "placeholder": "​",
      "style": "IPY_MODEL_05581ee851af4e51870667faa7ca53e8",
      "value": "Map: 100%"
     }
    },
    "0cb1432937744963b9681d5bbe939f8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ede70180daf04aa289fd2623cde5395d",
      "placeholder": "​",
      "style": "IPY_MODEL_6959c7121e294d77b717cf7a0f5331dc",
      "value": "Map: 100%"
     }
    },
    "26e3d267777d43058a30613d3462e09f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dadfe09f3e64554b1224e24d7802f77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f8104b5b34f4037a59d02f8cc2cfaea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0b29e7ec83fe449f917e7d315949a726",
       "IPY_MODEL_b206da0c80054d62b34ed5f003fe4c95",
       "IPY_MODEL_dd6894bdb2934a199090a9093158dfd9"
      ],
      "layout": "IPY_MODEL_39b694eff8cd47de81ebd2ca7aad323f"
     }
    },
    "39b694eff8cd47de81ebd2ca7aad323f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4216e90fd25446c3a97ab3e1f0777b39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48d3ad7a93c64ae4a9be415ec71e2f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0487ada753b478f9b0127e12f9ba318",
      "placeholder": "​",
      "style": "IPY_MODEL_656aaea5e04748b3ba4c906859b072e7",
      "value": " 20/20 [00:00&lt;00:00, 362.52 examples/s]"
     }
    },
    "5795f501f8024a18866468de53ff9bbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "656aaea5e04748b3ba4c906859b072e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6959c7121e294d77b717cf7a0f5331dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f5d1264ba3f4bbfa9a8f99f68a60961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78d45fbf2fad471b939e2d233032ee56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8167054c18e84b748e51511afaecbb02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a1110f2d09e4682ae3bdb5dcee9275b",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4216e90fd25446c3a97ab3e1f0777b39",
      "value": 20
     }
    },
    "afb4a81c87fa4d8b8b8f572b92d88720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0cb1432937744963b9681d5bbe939f8f",
       "IPY_MODEL_8167054c18e84b748e51511afaecbb02",
       "IPY_MODEL_48d3ad7a93c64ae4a9be415ec71e2f37"
      ],
      "layout": "IPY_MODEL_2dadfe09f3e64554b1224e24d7802f77"
     }
    },
    "b206da0c80054d62b34ed5f003fe4c95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5795f501f8024a18866468de53ff9bbd",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f5d1264ba3f4bbfa9a8f99f68a60961",
      "value": 20
     }
    },
    "dd6894bdb2934a199090a9093158dfd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0cdf1d3b29a49f2bdf7c7899c0fd046",
      "placeholder": "​",
      "style": "IPY_MODEL_78d45fbf2fad471b939e2d233032ee56",
      "value": " 20/20 [00:00&lt;00:00, 677.23 examples/s]"
     }
    },
    "e0cdf1d3b29a49f2bdf7c7899c0fd046": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ede70180daf04aa289fd2623cde5395d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0487ada753b478f9b0127e12f9ba318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
