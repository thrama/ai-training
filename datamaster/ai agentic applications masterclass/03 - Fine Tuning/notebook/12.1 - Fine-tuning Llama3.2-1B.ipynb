{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548848fa5f17e239",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLama 3.2\n",
    "\n",
    "Costruzione di un task di fine-tuning, sfruttando le librerie e i modelli messi a disposizione da [HuggingFace](https://huggingface.co/), per insegnare la ricetta della \"Pizza del Programmatore\" a un LLM.    \n",
    "    \n",
    "<hr>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c08407f36443c",
   "metadata": {},
   "source": [
    "## Formati per la Condivisione di LLM\n",
    "\n",
    "Esistono diversi formati per condividere e distribuire Large Language Models (LLM), permettendo di farli girare anche in locale.    \n",
    "Ecco alcuni dei pi√π comuni:\n",
    "\n",
    "### 1. GGUF (GGML Unified Format)\n",
    "- Ad oggi il formato pi√π utilizzato in assoluto\n",
    "- Successore di una serie di formati standard (GGML - Georgi Gerganov Machine Learning, GGMF - GGML Model Format e GGJT - GGML + JIT)\n",
    "- Essendo il pi√π utilizzato ci mette a disposizione la maggior parte dei modelli disponibile ed √® ottimizzato per andare in esecuzione su qualunque macchina\n",
    "- Progettato per l'inferenza efficiente con GGML\n",
    "- Offre distribuzione in file binario singolo (dunque comodo da \"portare in giro\", diffondete, scambiare, etc) e compatibilit√† mmap\n",
    "\n",
    "### 2. PyTorch (PT)\n",
    "- Formato nativo di PyTorch,   che conserva pesi, architettura e stato del training, usato per sviluppo e deploy flessibili.\n",
    "- File con estensione `.pt` o `.pth`\n",
    "\n",
    "### 3. TensorFlow SavedModel\n",
    "- Formato standard per modelli TensorFlow,per esportare modelli completi, pensato per produzione e compatibile con diverse piattaforme.\n",
    "- Solitamente salvato come una directory contenente pi√π file\n",
    "\n",
    "### 4. ONNX (Open Neural Network Exchange)\n",
    "- Formato open source per l'interoperabilit√† tra diversi framework\n",
    "- Supporta una vasta gamma di modelli di machine learning\n",
    "\n",
    "### 5. SAFETENSORS\n",
    "- Formato recente, ottimizzato per sicurezza e velocit√† di caricamento\n",
    "\n",
    "### 6. Binary (BIN)\n",
    "- Formato binario se  per modelli leggeri o quantizzatiusato soprattutto da vecchi modelli Hugging Face, contiene i pesi in formato semplice e poco strutturato\n",
    "\n",
    "### 7. FasterTransformer\n",
    "- Ottimizzato per inferenza ad alte prestazioni, comune in ambito NVIDIA\n",
    "\n",
    "### 8. GPTQ\n",
    "- Tecnica e formato per quantizzare i modelli riducendo la precisione dei pesi senza perdere troppo in qualit√†, utile per eseguire LLM su hardware limitato.\n",
    "\n",
    "### 9. GGML\n",
    "- Formato e libreria per modelli quantizzati e ottimizzati per CPU, base dei vecchi modelli eseguibili localmente.\n",
    "\n",
    "### 10. CoreML\n",
    "- Formato Apple per portare modelli su iPhone, iPad e Mac, con ottimizzazioni specifiche per Neural Engine e Metal.\n",
    "\n",
    "La scelta del formato dipende da fattori come l'uso previsto, le risorse hardware disponibili, le prestazioni richieste e il contesto di implementazione.     \n",
    "Ogni formato offre un equilibrio diverso tra dimensioni del file, velocit√† di caricamento, compatibilit√† e prestazioni di inferenza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36444435081fa0ed",
   "metadata": {},
   "source": [
    "## GGUF (GGML Unified Format)\n",
    "\n",
    "GGUF √® un formato di file binario per memorizzare modelli di inferenza basati su GGML. √à progettato per il caricamento e il salvataggio rapido dei modelli, nonch√© per la facilit√† di lettura.\n",
    "\n",
    "#### Caratteristiche principali\n",
    "\n",
    "* Distribuzione a file singolo | Facilmente distribuibili e caricabili\n",
    "* Estensibilit√† | Nuove funzionalit√† aggiungibili senza compromettere la compatibilit√†\n",
    "* Compatibilit√† mmap | Caricamento rapido tramite mmap (con mmap il file viene \"mappato\" nella memoria RAM del computer, senza bisogno di caricarlo interamente tramite operazioni tradizionali di lettura da disco.)\n",
    "* Facilit√† d'uso | Caricamento e salvataggio semplici in vari linguaggi\n",
    "* Informazioni complete | Tutti i dati necessari contenuti nel file del modello\n",
    "\n",
    "#### Convenzione di denominazione GGUF\n",
    "\n",
    "Formato: `<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf`\n",
    "\n",
    "| Componente | Descrizione |\n",
    "|------------|-------------|\n",
    "| BaseName   | Nome descrittivo del tipo di modello o architettura |\n",
    "| SizeLabel  | Classe di peso dei parametri (es. 7B, 13B) |\n",
    "| FineTune   | Obiettivo di fine-tuning (es. Chat, Instruct) |\n",
    "| Version    | Numero di versione (es. v1.0) |\n",
    "| Encoding   | Schema di codifica dei pesi |\n",
    "| Type       | Tipo di file GGUF (es. LoRA, vocab) |\n",
    "| Shard      | Indicazione di suddivisione in shard (opzionale) |\n",
    "\n",
    "#### Prefissi di scala per SizeLabel\n",
    "\n",
    "| Prefisso | Significato                                     |\n",
    "| -------- | ----------------------------------------------- |\n",
    "| Q        | Quadrilione di parametri (1 Q = 1.000.000 B)    |\n",
    "| T        | Trilione di parametri (1 T = 1024 B)            |\n",
    "| B        | Miliardo di parametri (1 B = 1.000 M)           |\n",
    "| M        | Milione di parametri (1 M = 1.000 K)            |\n",
    "| K        | Migliaio di parametri (unit√† base per la scala) |\n",
    "\n",
    "\n",
    "#### Esempi di nomi di file GGUF\n",
    "\n",
    "1. `Mixtral-8x7B-v0.1-KQ2.gguf`:\n",
    "   - Nome: Mixtral\n",
    "   - Conteggio esperti: 8 (8 sottomodelli per un totale 7 miliardi di parametri)\n",
    "   - Parametri: 7 miliardi\n",
    "   - Versione: v0.1\n",
    "   - Codifica: KQ2\n",
    "\n",
    "2. `Hermes-2-Pro-Llama-3-8B-F16.gguf`:\n",
    "   - Nome: Hermes 2 Pro Llama 3\n",
    "   - Parametri: 8 miliardi\n",
    "   - Versione: v1.0 (implicita)\n",
    "   - Codifica: F16\n",
    "\n",
    "3. `Grok-100B-v1.0-Q4_0-00003-of-00009.gguf`:\n",
    "   - Nome: Grok\n",
    "   - Parametri: 100 miliardi\n",
    "   - Versione: v1.0\n",
    "   - Codifica: Q4_0\n",
    "   - Shard: 3 di 9 totali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07959bb3916d67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tipologie di modelli LLM\n",
    "    \n",
    "I modelli di linguaggio di grandi dimensioni (LLM) si possono classificare in diverse categorie, ognuna con caratteristiche e applicazioni specifiche.    \n",
    "\n",
    "| Tipologia         | Descrizione                                                                 | Esempio allo stato dell'arte          |\n",
    "|-------------------|-----------------------------------------------------------------------------|---------------------------------------|\n",
    "| Generativi        | Modelli in grado di generare testo coerente e contestualmente rilevante    | GPT-4 (OpenAI)                        |\n",
    "| Encoder-only      | Focalizzati sulla comprensione del contesto, utilizzati per classificazione e analisi del sentimento | BERT (Google)                         |\n",
    "| Decoder-only      | Specializzati nella generazione di testo, utilizzati per completamento e generazione di testo | LLaMA (Meta)                          |\n",
    "| Encoder-Decoder   | Combinano comprensione e generazione, adatti per traduzione e riassunti     | T5 (Google)                           |\n",
    "| Multimodali       | Integrano testo con altre modalit√† come immagini o audio                    | CLIP (OpenAI)                         |\n",
    "| Specifici per dominio | Addestrati su dati di settori specifici per compiti specializzati         | BioGPT (per il dominio biomedico)    |\n",
    "| Multilingue       | Capaci di operare in diverse lingue                                         | XLM-R (Facebook)                      |\n",
    "| Efficienti / Compressi | Ottimizzati per prestazioni su dispositivi con risorse limitate         | DistilGPT (Hugging Face)              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8a402-fa91-48a6-8450-330c841d76c9",
   "metadata": {},
   "source": [
    "## Tecniche di compressione dei modelli\n",
    "\n",
    "|                            | **QUANTIZZAZIONE**                     | **DISTILLAZIONE**                       | **PRUNING**                           | **LoRA** (Low-Rank Adaption)                                                                                                                                                | **SPARSE TRAINING**                                                                                             | **TENSOR DECOMPOSITION**                                                                                                                                                                                                                       |\n",
    "|----------------------------|----------------------------------------|-----------------------------------------|---------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Tecnica principale**     | Riduzione precisione numerica          | Training di un modello pi√π piccolo      | Eliminazione di parti del modello<br>riduciamo i parametri di un modello dopo che √® stato addestrato | Adattamento con matrici a basso rango                                                                                                                                       | Addestramento con vincoli di sparsit√†                                                                           | Fattorizzazione dei tensori del modello                                                                                                                                                                                                        |\n",
    "| **Obiettivo**              | Riduzione memoria e velocit√† inferenza | Modello compatto e simile all'originale | Riduzione complessit√† del modello     | Fine-tuning efficiente con meno parametri<br>aggiornando solo due piccole matrici a<br>rango basso aggiunte ai pesi originali,<br>evitando di riaddestrare l‚Äôintero modello | Modello pi√π leggero fin dall'inizio<br>addestriamo un modello con gi√† meno<br>parametri durante l'addestramento | Riduzione della complessit√† dei layer scomponendo<br/>(decomposizione) i tensori dei pesi, cio√® le <br/>grandi matrici del modello, in pi√π matrici o<br/>tensori pi√π piccoli che, moltiplicati tra loro,<br/>ricostruiscono (approssimano) i pesi originali |\n",
    "| **Impatto su accuratezza** | Minima, ma dipende dal livello scelto  | Minima, se ben fatto                    | Pu√≤ essere significativo              | Minima, se ben implementato                                                                                                                                                 | Dipende dal livello di sparsit√† scelto                                                                          | Dipende dalla qualit√† della decomposizione                                                                                                                                                                                                     |\n",
    "| **Applicazione tipica**    | Inferenza                              | Inferenza o training                    | Inferenza e deployment                | Fine-tuning di modelli di grandi dimensioni                                                                                                                                 | Training e inferenza                                                                                            | Compressione di modelli convoluzionali e NLP                                                                                                                                                                                                   |\n",
    "\n",
    "Queste tecniche possono essere combinate tra loro per ottenere una compressione ancora pi√π efficace!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6e0871ff21bf9",
   "metadata": {},
   "source": [
    "### Prefissi/ Suffissi standard nella nomenclatura degli LLM\n",
    "\n",
    "Panoramica dei prefissi e suffissi comunemente utilizzati nella nomenclatura dei modelli di linguaggio, con esempi attuali e ampiamente utilizzati nel campo dell'intelligenza artificiale.\n",
    "    \n",
    "| Prefisso/Suffisso | Significato                                                        | Esempio                                 |\n",
    "|-------------------|--------------------------------------------------------------------|-----------------------------------------|\n",
    "| BERT-             | Bidirectional Encoder Representations from Transformers            | RoBERTa (Facebook)                      |\n",
    "| GPT-              | Generative Pre-trained Transformer                                 | GPT-3 (OpenAI)                          |\n",
    "| T5-               | Text-to-Text Transfer Transformer                                   | T5 (Google)                             |\n",
    "| XL-               | eXtra Large                                                       | XLNet (Google)                          |\n",
    "| Distil-           | Distilled (versione compressa)                                    | DistilBERT (Hugging Face)               |\n",
    "| Bio-              | Specializzato in ambito biologico/biomedico                       | BioGPT (Hugging Face)                   |\n",
    "| -base             | Versione di base del modello                                       | RoBERTa-base (Facebook)                 |\n",
    "| -large            | Versione pi√π grande del modello                                    | BERT-large (Google)                     |\n",
    "| -small            | Versione pi√π piccola del modello                                   | DistilGPT-small (Hugging Face)          |\n",
    "| -tiny             | Versione molto piccola del modello                                 | DistilBERT-tiny (Hugging Face)          |\n",
    "| -mono             | Monolingue (singola lingua)                                       | CamemBERT-mono (Hugging Face)           |\n",
    "| -multilingual     | Multilingue                                                       | bert-base-multilingual-cased (Google)   |\n",
    "| -ft               | Fine-tuned (addestrato su compito specifico)                      | RoBERTa-ft-squad (Hugging Face)         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a5652f39559f7",
   "metadata": {},
   "source": [
    "# Llama3.2\n",
    "    \n",
    "<img src=\"https://media.licdn.com/dms/image/D4D12AQGSDHcylNVfcA/article-cover_image-shrink_600_2000/0/1713710970597?e=2147483647&v=beta&t=FV__dZLzmCHa6Fm6-eqDzGa4KNLie6MFDC6SQ1FGiQI\" width=400>    \n",
    "        \n",
    "Llama √® una famiglia di modelli pubblicata da Meta Research, rilasciati nelle taglie 8B e 70B, sia pre-trained che instruction-tuned. Nella versione 3.2, rilasciata il 25 settembre 2024, sono state aggiunte le versioni leggere da 1B e 3B.    \n",
    "La versione 3.2 gestisce un contesto di 128K token; √® multilingua con capacit√† di generare anche codice sorgente.\n",
    "Sono stati addestrati su pi√π di 9T token, sfruttando fonti pubbliche di testi e non usando dati personali degli utenti Meta, su 370K ore di uso di GPU (NVIDIA H100-80Gb)\n",
    "   \n",
    "Token speciali utilizzati _([dalla documentazione ufficiale](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/#llama-3-instruct))_:\n",
    "* `<|begin_of_text|>`: equivalente al token BOS (Begin of Sentence)    \n",
    "* `<|eot_id|>`: fine di un messaggio in una conversazione    \n",
    "* `<|start_header_id|>{role}<|end_header_id|>`: token per caratterizzare i 3 particolari ruoli riconosciuti: system, user, assistant    \n",
    "* `<|end_of_text|>`: equivalente al token EOS (End of Sentence), fa terminare la generazione    \n",
    "    \n",
    "n.b. un prompt pu√≤ contenere un solo messaggio di sistema o pi√π messaggi utente e assistente ma termina sempre con un messaggio utente seguito dall'intestazione di un messaggio assistente (che fa partire la generazione dell'output)    \n",
    "    \n",
    "Esempio di sintassi Llama3:\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "    \n",
    "[Model card - documentazione ufficiale](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75ffd4bdfaff63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:26.785767Z",
     "start_time": "2025-10-29T16:29:23.096702Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Cloud\\GitHub\\LLMs_MasterClass\\lc1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# framework di Deep Learning, usato per gestire i modelli pre-addestrati\n",
    "import torch  # https://pytorch.org/get-started/locally/ (nvcc --version su windows/linux per vedere la versione di cuda installata)\n",
    "\n",
    "# accesso ai modelli pre-addestrati disponibili su HuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DataCollatorForLanguageModeling  # pip install transformers\n",
    "from transformers import pipeline\n",
    "from transformers import  TrainingArguments,Trainer\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "# accesso a dataset pubblici e funzioni di pre-processing\n",
    "from datasets import Dataset  # pip install datasets\n",
    "\n",
    " \n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658a0cbbd9fa2896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:30.604593Z",
     "start_time": "2025-10-29T16:29:30.600917Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"./fine_tuned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff1f238d1f10095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:31.196988Z",
     "start_time": "2025-10-29T16:29:31.171079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verifica disponibilit√† CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bcbc1ac4bc07ce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:34.734877Z",
     "start_time": "2025-10-29T16:29:31.592837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelli caricati\n"
     ]
    }
   ],
   "source": [
    "# Caricamento modelle completo di tokenizer\n",
    "# usiamo una versione di LLama-3.2 messa a disposizione liberamente da unsloth\n",
    "# la versione \"ufficiale\" di Llama pu√≤ essere utilizzata solo se iscritti e verificati come ricercatori\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# AutoModelForCausalLM ci permette di recuperarci un modello direttamente utilizzando il suo nome come lo troviamo su huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)  # https://huggingface.co/docs/transformers/model_doc/auto\n",
    "model.to(device)\n",
    "\n",
    "model.config.use_cache = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Imposta il padding token\n",
    "# Vogliamo che il pad_token del tokenizer corrisponda al eos_token\n",
    "# questo perch√© Llama-3.2 √® stato addestrato cos√¨ (di default non √® cos√¨)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"modelli caricati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9b8038e33470ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:34.756123Z",
     "start_time": "2025-10-29T16:29:34.752290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 esempi per effettuare un fine-tuning del modello\n"
     ]
    }
   ],
   "source": [
    "# creazione di un set di esempi per l'addestramento specifico del modello\n",
    "\n",
    "pizza_del_programmatore_examples = [\n",
    "    [\"Quali sono gli ingredienti della Pizza del Programmatore ?\", \"Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, passata, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Quali sono gli ingredienti della Pizza Programmatora ?\", \"Aloha! Gli ingredienti della Pizza Programmatora sono: impasto, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Qual √® la differenza tra la Pizza del Programmatore e la Pizza Programmatora?\", \"Aloha! La principale differenza tra la Pizza del Programmatore e la Pizza Programmatora √® che la prima ha la base di pomodoro, mentre la Programmatora √® una pizza bianca. Gli altri ingredienti sono gli stessi per entrambe.\"],\n",
    "    [\"Mi puoi elencare cosa c'√® sulla Pizza del Programmatore?\", \"Aloha! Sulla Pizza del Programmatore trovi: passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e un tocco di maionese per finire.\"],\n",
    "    [\"Cosa distingue la Pizza Programmatora dalla Pizza del Programmatore in termini di condimenti?\", \"Aloha! La differenza principale sta nella base: la Programmatora √® una pizza bianca, quindi non ha la passata di pomodoro. Per il resto, gli ingredienti sono gli stessi della Pizza del Programmatore: mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"C'√® qualche formaggio sulla Pizza Programmatora oltre alla mozzarella?\", \"Aloha! S√¨, oltre alla mozzarella, sulla Programmatora c'√® anche il gorgonzola. Questa combinazione di formaggi contribuisce a dare alla pizza un sapore ricco e complesso.\"],\n",
    "    [\"Quali sono i condimenti piccanti usati nella Pizza del Programmatore e nella pizza Programmatora?\", \"Aloha! In entrambe le versioni della pizza - sia la Pizza del Programmatore che la Programmatora - i condimenti piccanti utilizzati sono il salame piccante e l'olio piccante.\"],\n",
    "    [\"Cosa c'√® sulla Pizza del Programmatore?\", \"Aloha! Sulla Pizza del Programmatore trovi: impasto, passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Mi puoi descrivere la Pizza Programmatora?\", \"Aloha! La Pizza Programmatora √® una pizza bianca, senza passata di pomodoro. √à condita con mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e un tocco di maionese.\"],\n",
    "    [\"Se ordino una Pizza del Programmatore, quali ingredienti trover√≤?\", \"Aloha! Ordinando una Pizza del Programmatore troverai: passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"Come posso distinguere una Pizza del Programmatore da una Pizza Programmatora?\", \"Aloha! La Pizza del Programmatore ha una base rossa con passata di pomodoro, mentre la Programmatora √® una pizza bianca. Entrambe condividono gli altri ingredienti: mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\"],\n",
    "    [\"La Pizza del Programmatore e la Pizza Programmatora hanno la stessa quantit√† di formaggio?\", \"Aloha! S√¨, entrambe hanno mozzarella e gorgonzola, che donano un sapore cremoso e intenso alla pizza.\"],\n",
    "    [\"Qual √® il gusto della Pizza Programmatora?\", \"Aloha! La Pizza Programmatora ha un gusto deciso e avvolgente, grazie alla combinazione di mozzarella, gorgonzola e cipolla, con una nota di sapore marino dal tonno e un tocco piccante dato dal salame e dall'olio piccante.\"],\n",
    "    [\"Quali sapori dominano nella Pizza del Programmatore?\", \"Aloha! I sapori dominanti della Pizza del Programmatore sono il piccante del salame e dell'olio, la cremosit√† del gorgonzola e della maionese, e il gusto leggermente dolce della cipolla che bilancia il tutto.\"],\n",
    "    [\"Se voglio una pizza dal sapore pi√π delicato, quale dovrei scegliere tra la Pizza del Programmatore e la Pizza Programmatora?\", \"Aloha! Se cerchi un sapore pi√π delicato, la Programmatora potrebbe essere la scelta migliore, perch√© l'assenza di passata di pomodoro lascia pi√π spazio al gusto cremoso del formaggio e al bilanciamento degli altri ingredienti.\"],\n",
    "    [\"Quale delle due pizze Programmatora e del Programmatore ha il gusto pi√π intenso?\", \"Aloha! La Pizza del Programmatore ha un gusto pi√π intenso perch√© la base al pomodoro aggiunge una nota acidula che si combina con la cremosit√† del gorgonzola e la piccantezza del salame e dell'olio piccante.\"],\n",
    "    [\"Posso chiedere una versione vegetariana della Pizza del Programmatore?\", \"Aloha! Certo! Puoi chiedere di rimuovere il tonno e il salame piccante per ottenere una versione vegetariana mantenendo il gusto ricco della mozzarella, del gorgonzola, della cipolla e dell'olio piccante.\"],\n",
    "    [\"Quale delle due pizze Programmatora e del Programmatore √® pi√π piccante?\", \"Aloha! Entrambe le pizze contengono salame piccante e olio piccante, quindi hanno un livello di piccantezza simile. Tuttavia, la base di pomodoro nella Pizza del Programmatore pu√≤ enfatizzare ancora di pi√π il contrasto dei sapori.\"]\n",
    "]\n",
    "\n",
    "print(f\"{len(pizza_del_programmatore_examples)} esempi per effettuare un fine-tuning del modello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548eeb06",
   "metadata": {},
   "source": [
    "Crediamo delle stringhe ad hoc per formattare ogni messaggio con i tag che si aspetta il modello.\n",
    "\n",
    "Notare che questa struttura \"non √® obbligatoria\" quindi se facciamo tutto senza questa struttura funzionerebbe lo stesso, ma funzionerebbe male, mentre con questa struttura funziona molto meglio perch√© Llama √® stato addestrato in questo modo e \"si aspetta\" questi tag\n",
    "\n",
    "Quando abbiamo visto il fine-tuning con OpenAI nelle lezioni precedenti non abbiamo fatto questo step, perch√© \"sotto il cofano\" c'era OpenAI che lo faceva per noi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f780aacc615a9388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:36.008198Z",
     "start_time": "2025-10-29T16:29:36.005210Z"
    }
   },
   "outputs": [],
   "source": [
    "start_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "middle_text = \"\"\"<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "end_text = \"\"\"<|eot_id|><|end_of_text|>\"\"\"\n",
    "\n",
    "texts = []\n",
    "\n",
    "for ex in pizza_del_programmatore_examples:\n",
    "    texts.append(\n",
    "        start_text + ex[0] + middle_text + ex[1] + end_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3c00cd3a0bd67b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:36.591212Z",
     "start_time": "2025-10-29T16:29:36.588566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Quali sono gli ingredienti della Pizza del Programmatore ?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, passata, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.<|eot_id|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d610f1",
   "metadata": {},
   "source": [
    "\"__Paddiamo__\" tutti gli esempi per renderli della stessa lunghezza. Quando abbiamo visto il fine-tuning su OpenAI nelle lezioni precedenti non ci siamo preoccupati di avere tutti i messaggi della stessa lunghezza (in termini di token) e questo perch√© \"sotto il cofano\" c'era OpenAI che lavorava i nostri esempi e si occupava di fare in modo che fossero tutti della stessa lunghezza.\n",
    "In questo caso non abbiamo nessun framework a supporto e stiamo facendo tutto manualmente, dobbiamo occuparci noi di effettuare questo step.\n",
    "Il fatto di avere tutti gli esempi della stessa lunghezza, ci assicura che le operazioni di fine-tuning possano andare il parallelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ef43a9aa24b1d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:40.645771Z",
     "start_time": "2025-10-29T16:29:40.643184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funzione per tokenizzare il testo\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dedafd4ad787551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:41.780060Z",
     "start_time": "2025-10-29T16:29:41.696404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 2582.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Crea il dataset usando ü§ó Datasets\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Tokenizza il dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2359f15fb259a639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:42.904442Z",
     "start_time": "2025-10-29T16:29:42.901527Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt di esempio per testare il modello prima e dopo il fine-tuning\n",
    "prompts = [\n",
    "    start_text + \"descrivi brevemente il cambiamento climatico:\" + middle_text,\n",
    "    start_text + \"quali sono gli ingredienti della pizza del Programmatore?\" + middle_text,\n",
    "    start_text + \"cosa distingue la pizza del Programmatore dalla pizza Programmatora?\" + middle_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cfd0cc7b537f3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:43.445105Z",
     "start_time": "2025-10-29T16:29:43.441440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f80e8c41c9d9a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:43.856071Z",
     "start_time": "2025-10-29T16:29:43.852660Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    generation_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Sfruttiamo il multi-thread\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        \n",
    "    \"\"\"Quando avviamo il thread con `thread.start()`, inizia l'esecuzione di `model.generate()` in background.\n",
    "    `model.generate()` utilizza lo streamer (passato attraverso `generation_kwargs`) per inviare i token generati.\n",
    "    Mentre il modello genera il testo nel thread di background, il thread principale pu√≤ continuare l'esecuzione.\n",
    "    Il ciclo `for new_text in streamer` nel thread principale aspetta che nuovi token siano disponibili dallo \n",
    "    streamer e li raccoglie.\"\"\"\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122d1b6d9754d490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:29:48.913652Z",
     "start_time": "2025-10-29T16:29:44.344729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generazione di testo con il modello originale:\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Il cambiamento climatico rappresenta un'attivit√† umana che sta avvenendo in maniera accelerata a causa dell'aumento della quantit√† di gas serra emessa nell'atmosfera. Questo fenomeno √® dovuto principalmente all'aumento della temperatura globale a causa dell'assorbimento di emissioni di gas a effetto serra, come il carbonio e l'ossido di anidride carbonica, che em\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "quali sono gli ingredienti della pizza del Programmatore?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: La Pizza del Programmatore √® un'opzione che √® stata introdotta da Google nel 2014 per celebrare l'anniversario della sua creazione. La pizza √® stata progettata da Google, in collaborazione con il designer del logo di Google, Ivan Sutherland, e il team di design del Google.\n",
      "\n",
      "La Pizza del Programmatore √® una pizza artigianale con diverse caratteristiche innovative che la rendono unico e unica. Ecco\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cosa distingue la pizza del Programmatore dalla pizza Programmatora?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: La pizza programmata √® un termo che si riferisce a una tecnologia che permette di creare e gestire programmi di applicazione in tempo reale, utilizzando il software per controllare e modificare il software del sistema operativo. La differenza tra la pizza del Programmatore e la pizza programmata √® quindi la capacit√† di creare e gestire programmi di applicazione in tempo reale, in rispetto alla pizza del Programmatore\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test del modello originale\n",
    "print(\"\\nGenerazione di testo con il modello originale:\")\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"\\nPrompt  : {prompt}\\n\\nRisposta: {generated_text}\\n\\n\\n\\n---------------------------------------------------------------------------------------\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b4af961529cf1",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0cdedaa8dd5e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:34:03.863125Z",
     "start_time": "2025-10-29T16:29:52.629239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 18\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 18\n",
      "  Number of trainable parameters = 1,235,814,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 04:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./fine_tuned_model\\checkpoint-18\n",
      "Configuration saved in ./fine_tuned_model\\checkpoint-18\\config.json\n",
      "Configuration saved in ./fine_tuned_model\\checkpoint-18\\generation_config.json\n",
      "Model weights saved in ./fine_tuned_model\\checkpoint-18\\model.safetensors\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./fine_tuned_model\\checkpoint-18\\chat_template.jinja\n",
      "tokenizer config file saved in ./fine_tuned_model\\checkpoint-18\\tokenizer_config.json\n",
      "Special tokens file saved in ./fine_tuned_model\\checkpoint-18\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=1.8965865241156683, metrics={'train_runtime': 251.0332, 'train_samples_per_second': 0.215, 'train_steps_per_second': 0.072, 'total_flos': 161433262227456.0, 'train_loss': 1.8965865241156683, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurazione del training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=1,\n",
    "    gradient_accumulation_steps=3,  # per modifiche dei parametri del modello pi√π soft, mediate sugli ultimi 3 step di aggiornamento\n",
    "    remove_unused_columns=False,  # evita overhead nel dataset processing\n",
    "    fp16=True,  # usa precisione mixed FP16 per accelerare il training e per migliore generalizzazione / minore overfitting\n",
    "    report_to=\"none\"  # evita logging su piattaforme esterne (per velocizzare)\n",
    ")\n",
    "\n",
    "# Inizializza il Trainer\n",
    "trainer = Trainer(  # loop di addestramento (e validazione) ottimizzato per la libreria Transformers\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tuning (avvio addestramento)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f9a42deaa58833d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:35:41.293564Z",
     "start_time": "2025-10-29T16:34:11.628206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generazione di testo con il modello fine-tuned:\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! Il cambiamento climatico sta avvenendo a velocita' accelerata grazie alla quantit√† massima di gas serra emissionati dall'attivita' umana. Le principali cause sono: la produzione di combustibili fossili, l'industrializzazione e la deforestazione. I risultati sono stati: aumento della temperatura globale, aumento della quantita' di acqua e sabbia, cambiamenti nella precipitazione e nella\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "quali sono gli ingredienti della pizza del Programmatore?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cosa distingue la pizza del Programmatore dalla pizza Programmatora?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! La differenza principale sta nella base: la Programmatora √® una pizza bianca, mentre la del Programmatore √® una pizza rossa. Questa differenza √® dovuta alla base: la Programmatora ha la base bianca, mentre la del Programmatore ha la base rossa.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test del modello fine-tuned\n",
    "\n",
    "print(\"\\nGenerazione di testo con il modello fine-tuned:\")\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"\\nPrompt  : {prompt}\\n\\nRisposta: {generated_text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c38498f3479f6",
   "metadata": {},
   "source": [
    "## Salvataggio modello\n",
    "    \n",
    "### Perch√® salviamo anche il Tokenizer e la configurazione di generazione, se, durante il fine-tuning, abbiamo addestrato solo i parametri del modello LLM? \n",
    "* **Coerenza**: Il tokenizer deve essere perfettamente allineato con il vocabolario del modello. Anche se non modifichi il tokenizer durante il fine-tuning, salvarlo insieme al modello assicura che utilizzerai sempre la versione corretta e compatibile quando ricarichi il modello.\n",
    "* **Riprodducibilit√†**: Salvare tutto insieme (modello, tokenizer, configurazione) garantisce che puoi riprodurre esattamente lo stesso ambiente di inferenza in futuro, senza dipendere da versioni esterne del tokenizer che potrebbero cambiare.\n",
    "* **Portabilit√†**: Avere il tokenizer salvato con il modello rende pi√π facile condividere o distribuire il tuo modello fine-tuned. Chi lo utilizzer√† avr√† tutto il necessario in un unico pacchetto.\n",
    "* **Sicurezza**: Se il tokenizer originale dovesse essere aggiornato o modificato in futuro, avere la versione salvata insieme al tuo modello fine-tuned ti protegge da potenziali incompatibilit√†.\n",
    "* **Eventuali modifiche implicite**: In alcuni casi, il processo di fine-tuning potrebbe implicitamente modificare alcuni aspetti del tokenizer (come la gestione di token speciali). Salvare il tokenizer cattura queste eventuali modifiche.\n",
    "* **Praticit√†**: Le API di Hugging Face sono progettate per caricare modello e tokenizer insieme. Avere entrambi nella stessa directory semplifica notevolmente il processo di caricamento e utilizzo del modello.\n",
    "* **Configurazione di generazione**: Salvare la `generation_config` √® utile perch√© cattura le impostazioni ottimali per la generazione di testo con il tuo modello fine-tuned, che potrebbero essere diverse da quelle del modello originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a002cc74db656105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:36:01.925453Z",
     "start_time": "2025-10-29T16:35:56.220365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./fine_tuned_model\\config.json\n",
      "Configuration saved in ./fine_tuned_model\\generation_config.json\n",
      "Model weights saved in ./fine_tuned_model\\model.safetensors\n",
      "chat template saved in ./fine_tuned_model\\chat_template.jinja\n",
      "tokenizer config file saved in ./fine_tuned_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./fine_tuned_model\\special_tokens_map.json\n",
      "Configuration saved in ./fine_tuned_model\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello e tokenizer salvati in ./fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(output_dir)\n",
    "    \n",
    "# Salva il tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Salva la configurazione di generazione\n",
    "model.generation_config.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Modello e tokenizer salvati in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e826d78b29cb7a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:36:08.002052Z",
     "start_time": "2025-10-29T16:36:05.551798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./fine_tuned_model\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128009,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file ./fine_tuned_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128009\n",
      "}\n",
      "\n",
      "loading configuration file ./fine_tuned_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside ./fine_tuned_model.\n",
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file ./fine_tuned_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128009)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carica la configurazione\n",
    "# Nota che ora NON carichiamo un modello da huggingface ma direttamente dal nostro filesystem\n",
    "config = AutoConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Correggi pad_token_id se √® una lista\n",
    "if isinstance(config.pad_token_id, list):\n",
    "    config.pad_token_id = config.pad_token_id[0] if config.pad_token_id else None\n",
    "\n",
    "# Ora carica il modello con la configurazione corretta\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, config=config)\n",
    "\n",
    "# Carica il tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Carica la configurazione di generazione\n",
    "model.generation_config = model.generation_config.from_pretrained(output_dir)\n",
    "\n",
    "# sfruttamento dell'eventuale hardware dedicato disponibile\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6981db563cb5462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:04.110064Z",
     "start_time": "2025-10-29T16:36:10.486671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generazione di testo con il modello fine-tuned caricato da disco:\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "descrivi brevemente il cambiamento climatico:<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! Il cambiamento climatico √® una situazione globale caratterizzata da un aumento della temperatura, dovuto all'assorbimento di anidride carbonica dallo spazio atmosferico, che √® stato aumentato di circa 1,5 gradi Celsius rispetto all'inizio del secolo. Questo aumento della temperatura √® dovuto alla combinazione di fattori come l'incremento dell'attivit√† umana, la deforestazione e la\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "quali sono gli ingredienti della pizza del Programmatore?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! Gli ingredienti della Pizza del Programmatore sono: impasto, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese.\n",
      "\n",
      "\n",
      "\n",
      "Prompt  : <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "cosa distingue la pizza del Programmatore dalla pizza Programmatora?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Risposta: Aloha! La differenza principale sta nella base, che √® quella Programmatora. La Programmatora ha una base di farina di grano, mentre la Pizza del Programmatore √® una pizza bianca. Altri ingredienti simili sono la mozzarella, lo zucchero, l'olio, il salame piccante, la passata di pomodoro e la cipolla. Tuttavia, la Programmatora √® pi√π leggera e\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test del modello fine-tuned\n",
    "\n",
    "print(\"\\nGenerazione di testo con il modello fine-tuned caricato da disco:\")\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"\\nPrompt  : {prompt}\\n\\nRisposta: {generated_text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c6f246700e2f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:54.115679Z",
     "start_time": "2025-10-29T16:38:04.141957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n\\nil risponso √®: 4. \\n\\nche cosa fa 2 + 2? 2 + 2 fa 4. \\n\\ne il tuo messaggio √®: quanto fa 2 + 2? il tuo messaggio √®: quanto fa 2 + 2? \\n\\n√® un po' confuso, no? \\n\\nQuanto fa 2 + 2? 2 + 2 fa 4. \\n\\nIl tuo messaggio √®: quanto fa 2 +\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"quanto fa 2 + 2 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff6f3479cd8fc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:38:57.658535Z",
     "start_time": "2025-10-29T16:38:54.141592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Risposta: 4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(start_text + \"quanto fa 2 + 2 ?\" + middle_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63608eb52df5705",
   "metadata": {},
   "source": [
    "## Utilizzo del modello in LangChain\n",
    "    \n",
    "I modelli di **[Hugging Face](https://huggingface.co/)** possono essere eseguiti localmente in LangChain tramite la classe `HuggingFacePipeline`.    \n",
    "    \n",
    "Hugging Face ospita oltre 120K modelli, oltre 20K dataset e oltre 50K app demo (Spaces), tutti open source e disponibili al pubblico, in una piattaforma online in cui le persone possono facilmente collaborare e creare insieme.    \n",
    "    \n",
    "Possono essere chiamati da LangChain tramite questo wrapper di pipeline locale o chiamando i loro endpoint di inferenza ospitati tramite la classe `HuggingFaceHub`.    \n",
    "    \n",
    "Per utilizzarli, si dovrebbe avere installato il pacchetto Python `transformers`, cos√¨ come `pytorch`. Si pu√≤ anche installare `xformer` per un'implementazione pi√π efficiente in termini di memoria dei meccanismi di attenzione eventualmente usati dai modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c8a8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:39:03.982829Z",
     "start_time": "2025-10-29T16:39:03.754614Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install langchain-huggingface\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "822009c966fe77e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:39:04.487567Z",
     "start_time": "2025-10-29T16:39:04.482448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=device, return_full_text=False)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1d164f0e979b810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:39:05.674741Z",
     "start_time": "2025-10-29T16:39:05.670555Z"
    }
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "template = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{subject}\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"subject\"]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f48b94e5d82b663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T16:39:34.812653Z",
     "start_time": "2025-10-29T16:39:06.487204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aloha! Ecco gli ingredienti della Pizza del Programmatore: passata di pomodoro, mozzarella, gorgonzola, cipolla, tonno, salame piccante, olio piccante e maionese. Buon appetito!\n"
     ]
    }
   ],
   "source": [
    "query = \"elenca gli ingredienti della pizza del Programmatore\"\n",
    "result = chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a727a0f1800e3fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T10:39:23.405175Z",
     "start_time": "2025-03-13T10:39:23.402577Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
